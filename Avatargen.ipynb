{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B2hVQNVpTjAc",
        "jUBt5lRHUpyL",
        "fwV2YC_pZ5-9",
        "uNrnG4PCr_3K"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Usage:\n",
        "Upload any picture into the /content folder and enter its path on the variables below.\n",
        "Then enter the gender as male, female or neutral\n",
        "Then you can just run all cells and it should work."
      ],
      "metadata": {
        "id": "MJieQ3CUoFQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_path = '/content/scholz.jpg'\n",
        "person_gender = 'male' # male, female or neutral\n",
        "image_name, image_type = os.path.splitext(os.path.basename(image_file_path))"
      ],
      "metadata": {
        "id": "hRWn1Aeya9FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download needed files from GoogleDrive"
      ],
      "metadata": {
        "id": "CA_dfFSWvxm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "XIEV7tSQre7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown -q --id 10edTLv6kcFZjXbS3tl9yyedW3nvPCtc5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAVMO2Qr1o5O",
        "outputId": "2317a760-858d-4507-9ef4-e3a0c2b2f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q AvatarGen_Part1\n",
        "!rm AvatarGen_Part1.zip"
      ],
      "metadata": {
        "id": "1Iy9mUwz2w80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('AvatarGen_Part1')"
      ],
      "metadata": {
        "id": "NKPovsxC3C_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download and import modules"
      ],
      "metadata": {
        "id": "tNFScU_iTx3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgS1qH7x9D20",
        "outputId": "b66dd6d5-08f3-4fc3-9573-d9613b0ada2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (0.29.36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastcore -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is6SPwx0-LgR",
        "outputId": "6ba81634-06b4-4c4e-b773-b7e018d710b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (1.5.29)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-cache-dir lpips Ninja open3d==0.16 smplx yacs flatten_dict pytorch-lightning chumpy trimesh rtree paddlepaddle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-g7RVdwTxVa",
        "outputId": "faf72453-76d0-416a-ebff-0f45c4dea860",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.5/422.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m228.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m271.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.3/686.3 kB\u001b[0m \u001b[31m225.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m386.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 MB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m265.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m308.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m265.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m196.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m213.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m182.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m387.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/royorel/StyleSDF.git\n",
        "%cd StyleSDF\n",
        "!pip install -r requirements.txt\n",
        "!python download_models.py\n",
        "%cd .."
      ],
      "metadata": {
        "id": "-vVmLMRLRdMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131fa55a-273a-4672-97d1-17517cc9f13c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleSDF'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 120 (delta 58), reused 93 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 3.02 MiB | 23.58 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "/content/StyleSDF\n",
            "Collecting lmdb (from -r requirements.txt (line 1))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
            "Collecting ninja (from -r requirements.txt (line 3))\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (9.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.66.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.10.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.19.3)\n",
            "Collecting scikit-video (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trimesh[easy] (from -r requirements.txt (line 10))\n",
            "  Downloading trimesh-3.23.1-py3-none-any.whl (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.3/686.3 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configargparse (from -r requirements.txt (line 11))\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting munch (from -r requirements.txt (line 12))\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting wandb (from -r requirements.txt (line 13))\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 5)) (2023.7.22)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2023.7.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 8)) (23.1)\n",
            "Collecting rtree (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading Rtree-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (67.7.2)\n",
            "Collecting pycollada (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading pycollada-0.7.2.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (4.19.0)\n",
            "Collecting colorlog (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting svg.path (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading svg.path-6.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (4.0.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (2.0.1)\n",
            "Collecting embreex (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading embreex-2.17.7.post1-cp310-cp310-manylinux_2_28_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from trimesh[easy]->-r requirements.txt (line 10)) (4.9.3)\n",
            "Collecting mapbox-earcut (from trimesh[easy]->-r requirements.txt (line 10))\n",
            "  Downloading mapbox_earcut-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 13))\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 13))\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 13))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (6.0.1)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 13))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 13))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 13)) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 13)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh[easy]->-r requirements.txt (line 10)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh[easy]->-r requirements.txt (line 10)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh[easy]->-r requirements.txt (line 10)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh[easy]->-r requirements.txt (line 10)) (0.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from pycollada->trimesh[easy]->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->trimesh[easy]->-r requirements.txt (line 10)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 13))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools, pycollada\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e4a566bbcdd08066c71982ef27ef5cf7799ad1745c2e4f4585b242134b1b1007\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "  Building wheel for pycollada (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycollada: filename=pycollada-0.7.2-py3-none-any.whl size=127016 sha256=1e81dd97ff49fed82390334c8fe13ff19cbb6ba7bcae585c50189ee3f102a9a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/ba/33/1e99a7e7defd1d77f0210e7a39ff58de2a2d8d4c22466bb2da\n",
            "Successfully built pathtools pycollada\n",
            "Installing collected packages: pathtools, ninja, lmdb, xxhash, trimesh, svg.path, smmap, setproctitle, sentry-sdk, rtree, munch, mapbox-earcut, embreex, docker-pycreds, configargparse, colorlog, scikit-video, pycollada, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 colorlog-6.7.0 configargparse-1.7 docker-pycreds-0.4.0 embreex-2.17.7.post1 gitdb-4.0.10 lmdb-1.4.1 mapbox-earcut-1.0.1 munch-4.0.0 ninja-1.11.1 pathtools-0.1.2 pycollada-0.7.2 rtree-1.0.1 scikit-video-1.1.11 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 svg.path-6.3 trimesh-3.23.1 wandb-0.15.8 xxhash-3.3.0\n",
            "Downloading sphere initialized volume renderer\n",
            "100% 63.7M/63.7M [00:01<00:00, 37.2MB/s]\n",
            "Downloading FFHQ pretrained volume renderer\n",
            "100% 63.7M/63.7M [00:01<00:00, 51.3MB/s]\n",
            "Downloading FFHQ full model (1024x1024)\n",
            "100% 203M/203M [00:02<00:00, 80.2MB/s]\n",
            "Done!\n",
            "Downloading AFHQ pretrained volume renderer\n",
            "100% 63.7M/63.7M [00:01<00:00, 54.1MB/s]\n",
            "Done!\n",
            "Downloading Downloading AFHQ full model (512x512)\n",
            "100% 185M/185M [00:02<00:00, 88.4MB/s]\n",
            "Done!\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this due to whatever resaon breaks, just run it again. If it still not working restart the runtime and try again.\n",
        "\n",
        "If this is also not working, for me it works when I close the tab and restart google colab fully\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Yoc2Sj1Y2hqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir sklearn"
      ],
      "metadata": {
        "id": "VGTei-sG1AbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97d7278-27be-4976-ae19-9a777dc5e52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=f233a451e670a1b59341c056c8648c0a8c210c332b33c9f5c610236aea3db166\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-96omw_p0/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q paddleseg==2.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9CY2spm94HK",
        "outputId": "d6a259d3-4a2d-4ac8-9610-9970f4175b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.9/295.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpCD3oUcrDn",
        "outputId": "a0f7e35c-b983-4d56-877f-c42a5af28bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dnnlib==0.0.1\n",
            "  Downloading https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: dnnlib\n",
            "Successfully installed dnnlib-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mkocabas/yolov3-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBbxjOFml8zc",
        "outputId": "93adcd70-0da7-45c2-9aec-857d290ae53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mkocabas/yolov3-pytorch.git\n",
            "  Cloning https://github.com/mkocabas/yolov3-pytorch.git to /tmp/pip-req-build-r4ac1cco\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mkocabas/yolov3-pytorch.git /tmp/pip-req-build-r4ac1cco\n",
            "  Resolved https://github.com/mkocabas/yolov3-pytorch.git to commit d7192c9f6cec43f7db4bd91186b2c0401eafd5a6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: yolov3\n",
            "  Building wheel for yolov3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yolov3: filename=yolov3-0.1-py3-none-any.whl size=25798 sha256=7e9fc59da9ca39ee4958b0ce5e049d48728de95fe5e667366936a63428a3878a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nsgbd7se/wheels/23/ab/7b/4235cf15b1a754dc81deb34b968ed01c27da7807b36c3d1dec\n",
            "Successfully built yolov3\n",
            "Installing collected packages: yolov3\n",
            "Successfully installed yolov3-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install -q fvcore iopath\n",
        "!pip install -q --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ikAUfZcx8H",
        "outputId": "e0cb007a-9c6e-4820-a83c-2364c73effef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m213.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n",
        "\n",
        "!dpkg -i libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb"
      ],
      "metadata": {
        "id": "wTiLnIi2SX0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32ca73a-ab0e-49f2-ee97-0482027851f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-14 20:21:25--  http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n",
            "Resolving nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)... 91.189.91.83, 185.125.190.39, 91.189.91.82, ...\n",
            "Connecting to nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)|91.189.91.83|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1321244 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M   835KB/s    in 1.5s    \n",
            "\n",
            "2023-08-14 20:21:27 (835 KB/s) - ‘libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb’ saved [1321244/1321244]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 120828 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2.19) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2.19) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --no-cache-dir install pillow==9.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7_cMzcnpym",
        "outputId": "5acfebd6-1370-434c-c2b4-23d3d2748f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import copy\n",
        "import cv2\n",
        "import argparse\n",
        "import yaml\n",
        "import imutils\n",
        "import codecs\n",
        "import abc\n",
        "import trimesh\n",
        "\n",
        "from random import choice\n",
        "from collections import OrderedDict\n",
        "from string import ascii_uppercase\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from PIL import Image\n",
        "from lpips import LPIPS\n",
        "from argparse import Namespace\n",
        "\n",
        "from yolov3.yolo import YOLOv3\n",
        "\n",
        "import paddle\n",
        "from paddle.inference import create_predictor, PrecisionType\n",
        "from paddle.inference import Config as PredictConfig\n",
        "from paddleseg.core.infer import reverse_transform\n",
        "from paddleseg.cvlibs import manager\n",
        "from paddleseg.utils import TimeAverager\n",
        "import paddleseg.transforms as T"
      ],
      "metadata": {
        "id": "SInuXRqEVbb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## align image"
      ],
      "metadata": {
        "id": "B2hVQNVpTjAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Align IMage module is taken from StyleHuman, can be found here:\n",
        "https://github.com/stylegan-human/StyleGAN-Human/blob/main/alignment.py\n",
        "\n",
        "Utils creates the needed helper function to use the Skript on the notebook"
      ],
      "metadata": {
        "id": "b9o8Z_YYWCKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "AbKhLk1wTEUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openpose.src import util\n",
        "from openpose.src.body import Body"
      ],
      "metadata": {
        "id": "F2UNIXIQTUUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ01hQoRRtmW"
      },
      "outputs": [],
      "source": [
        "def human_seg_tracking(pre_gray, cur_gray, prev_cfd, dl_weights, disflow):\n",
        "\n",
        "    check_thres = 8\n",
        "    h, w = pre_gray.shape[:2]\n",
        "    track_cfd = np.zeros_like(prev_cfd)\n",
        "    is_track = np.zeros_like(pre_gray)\n",
        "    flow_fw = disflow.calc(pre_gray, cur_gray, None)\n",
        "    flow_bw = disflow.calc(cur_gray, pre_gray, None)\n",
        "    flow_fw = np.round(flow_fw).astype(np.int)\n",
        "    flow_bw = np.round(flow_bw).astype(np.int)\n",
        "    y_list = np.array(range(h))\n",
        "    x_list = np.array(range(w))\n",
        "    yv, xv = np.meshgrid(y_list, x_list)\n",
        "    yv, xv = yv.T, xv.T\n",
        "    cur_x = xv + flow_fw[:, :, 0]\n",
        "    cur_y = yv + flow_fw[:, :, 1]\n",
        "\n",
        "    not_track = (cur_x < 0) + (cur_x >= w) + (cur_y < 0) + (cur_y >= h)\n",
        "    flow_bw[~not_track] = flow_bw[cur_y[~not_track], cur_x[~not_track]]\n",
        "    not_track += (np.square(flow_fw[:, :, 0] + flow_bw[:, :, 0]) +\n",
        "                  np.square(flow_fw[:, :, 1] + flow_bw[:, :, 1])) >= check_thres\n",
        "    track_cfd[cur_y[~not_track], cur_x[~not_track]] = prev_cfd[~not_track]\n",
        "\n",
        "    is_track[cur_y[~not_track], cur_x[~not_track]] = 1\n",
        "\n",
        "    not_flow = np.all(np.abs(flow_fw) == 0,\n",
        "                      axis=-1) * np.all(np.abs(flow_bw) == 0, axis=-1)\n",
        "    dl_weights[cur_y[not_flow], cur_x[not_flow]] = 0.05\n",
        "    return track_cfd, is_track, dl_weights\n",
        "\n",
        "def human_seg_track_fuse(track_cfd, dl_cfd, dl_weights, is_track):\n",
        "    fusion_cfd = dl_cfd.copy()\n",
        "    is_track = is_track.astype(np.bool)\n",
        "    fusion_cfd[is_track] = dl_weights[is_track] * dl_cfd[is_track] + (\n",
        "        1 - dl_weights[is_track]) * track_cfd[is_track]\n",
        "    index_certain = ((dl_cfd > 0.9) + (dl_cfd < 0.1)) * is_track\n",
        "    index_less01 = (dl_weights < 0.1) * index_certain\n",
        "    fusion_cfd[index_less01] = 0.3 * dl_cfd[index_less01] + 0.7 * track_cfd[\n",
        "        index_less01]\n",
        "    index_larger09 = (dl_weights >= 0.1) * index_certain\n",
        "    fusion_cfd[index_larger09] = 0.4 * dl_cfd[index_larger09] + 0.6 * track_cfd[\n",
        "        index_larger09]\n",
        "    return fusion_cfd\n",
        "\n",
        "\n",
        "def optic_flow_process(cur_gray, scoremap, prev_gray, pre_cfd, disflow,\n",
        "                       is_init):\n",
        "    h, w = scoremap.shape\n",
        "    cur_cfd = scoremap.copy()\n",
        "\n",
        "    if is_init:\n",
        "        if h <= 64 or w <= 64:\n",
        "            disflow.setFinestScale(1)\n",
        "        elif h <= 160 or w <= 160:\n",
        "            disflow.setFinestScale(2)\n",
        "        else:\n",
        "            disflow.setFinestScale(3)\n",
        "        fusion_cfd = cur_cfd\n",
        "    else:\n",
        "        weights = np.ones((h, w), np.float32) * 0.3\n",
        "        track_cfd, is_track, weights = human_seg_tracking(\n",
        "            prev_gray, cur_gray, pre_cfd, weights, disflow)\n",
        "        fusion_cfd = human_seg_track_fuse(track_cfd, cur_cfd, weights, is_track)\n",
        "\n",
        "    return fusion_cfd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff'\n",
        "]\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def make_dataset(dir):\n",
        "    images = []\n",
        "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in fnames:\n",
        "            if is_image_file(fname):\n",
        "                path = os.path.join(root, fname)\n",
        "                fname = fname.split('.')[0]\n",
        "                images.append((fname, path))\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImagesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, source_root, source_transform=None):\n",
        "        self.source_paths = sorted(make_dataset(source_root))\n",
        "        self.source_transform = source_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fname, from_path = self.source_paths[index]\n",
        "        from_im = Image.open(from_path).convert('RGB')\n",
        "\n",
        "        if self.source_transform:\n",
        "            from_im = self.source_transform(from_im)\n",
        "\n",
        "        return fname, from_im"
      ],
      "metadata": {
        "id": "8kbTwZD7TKrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeployConfig:\n",
        "    def __init__(self, path):\n",
        "        with codecs.open(path, 'r', 'utf-8') as file:\n",
        "            self.dic = yaml.load(file, Loader=yaml.FullLoader)\n",
        "\n",
        "        self._transforms = self._load_transforms(self.dic['Deploy'][\n",
        "            'transforms'])\n",
        "        self._dir = os.path.dirname(path)\n",
        "\n",
        "    @property\n",
        "    def transforms(self):\n",
        "        return self._transforms\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return os.path.join(self._dir, self.dic['Deploy']['model'])\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return os.path.join(self._dir, self.dic['Deploy']['params'])\n",
        "\n",
        "    def _load_transforms(self, t_list):\n",
        "        com = manager.TRANSFORMS\n",
        "        transforms = []\n",
        "        for t in t_list:\n",
        "            ctype = t.pop('type')\n",
        "            transforms.append(com[ctype](**t))\n",
        "\n",
        "        return transforms\n",
        "\n",
        "\n",
        "class PP_HumenSeg_Predictor:\n",
        "    def __init__(self, args):\n",
        "        self.cfg = DeployConfig(args.cfg)\n",
        "        self.args = args\n",
        "        self.compose = T.Compose(self.cfg.transforms)\n",
        "        resize_h, resize_w = args.input_shape\n",
        "\n",
        "        self.disflow = cv2.DISOpticalFlow_create(\n",
        "            cv2.DISOPTICAL_FLOW_PRESET_ULTRAFAST)\n",
        "        self.prev_gray = np.zeros((resize_h, resize_w), np.uint8)\n",
        "        self.prev_cfd = np.zeros((resize_h, resize_w), np.float32)\n",
        "        self.is_init = True\n",
        "\n",
        "        pred_cfg = PredictConfig(self.cfg.model, self.cfg.params)\n",
        "        pred_cfg.disable_glog_info()\n",
        "        if self.args.use_gpu:\n",
        "            pred_cfg.enable_use_gpu(100, 0)\n",
        "\n",
        "        self.predictor = create_predictor(pred_cfg)\n",
        "        if self.args.test_speed:\n",
        "            self.cost_averager = TimeAverager()\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        ori_shapes = []\n",
        "        processed_imgs = []\n",
        "        processed_img = self.compose(img)[0]\n",
        "        processed_imgs.append(processed_img)\n",
        "        ori_shapes.append(img.shape)\n",
        "        return processed_imgs, ori_shapes\n",
        "\n",
        "    def run(self, img, bg):\n",
        "        input_names = self.predictor.get_input_names()\n",
        "        input_handle = self.predictor.get_input_handle(input_names[0])\n",
        "        processed_imgs, ori_shapes = self.preprocess(img)\n",
        "        data = np.array(processed_imgs)\n",
        "        input_handle.reshape(data.shape)\n",
        "        input_handle.copy_from_cpu(data)\n",
        "\n",
        "        self.predictor.run()\n",
        "\n",
        "        output_names = self.predictor.get_output_names()\n",
        "        output_handle = self.predictor.get_output_handle(output_names[0])\n",
        "        output = output_handle.copy_to_cpu()\n",
        "        return self.postprocess(output, img, ori_shapes[0], bg)\n",
        "\n",
        "\n",
        "    def postprocess(self, pred, img, ori_shape, bg):\n",
        "        resize_w = pred.shape[-1]\n",
        "        resize_h = pred.shape[-2]\n",
        "        if self.args.soft_predict:\n",
        "            if self.args.use_optic_flow:\n",
        "                score_map = pred[:, 1, :, :].squeeze(0)\n",
        "                score_map = 255 * score_map\n",
        "                cur_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                cur_gray = cv2.resize(cur_gray, (resize_w, resize_h))\n",
        "                optflow_map = optic_flow_process(cur_gray, score_map, self.prev_gray, self.prev_cfd, \\\n",
        "                        self.disflow, self.is_init)\n",
        "                self.prev_gray = cur_gray.copy()\n",
        "                self.prev_cfd = optflow_map.copy()\n",
        "                self.is_init = False\n",
        "\n",
        "                score_map = np.repeat(optflow_map[:, :, np.newaxis], 3, axis=2)\n",
        "                score_map = np.transpose(score_map, [2, 0, 1])[np.newaxis, ...]\n",
        "                score_map = reverse_transform(\n",
        "                    paddle.to_tensor(score_map),\n",
        "                    ori_shape,\n",
        "                    self.cfg.transforms,\n",
        "                    mode='bilinear')\n",
        "                alpha = np.transpose(score_map.numpy().squeeze(0),\n",
        "                                     [1, 2, 0]) / 255\n",
        "            else:\n",
        "                score_map = pred[:, 1, :, :]\n",
        "                score_map = score_map[np.newaxis, ...]\n",
        "                score_map = reverse_transform(\n",
        "                    paddle.to_tensor(score_map),\n",
        "                    ori_shape,\n",
        "                    self.cfg.transforms,\n",
        "                    mode='bilinear')\n",
        "                alpha = np.transpose(score_map.numpy().squeeze(0), [1, 2, 0])\n",
        "\n",
        "        else:\n",
        "            if pred.ndim == 3:\n",
        "                pred = pred[:, np.newaxis, ...]\n",
        "            result = reverse_transform(\n",
        "                paddle.to_tensor(\n",
        "                    pred, dtype='float32'),\n",
        "                ori_shape,\n",
        "                self.cfg.transforms,\n",
        "                mode='bilinear')\n",
        "\n",
        "            result = np.array(result)\n",
        "            if self.args.add_argmax:\n",
        "                result = np.argmax(result, axis=1)\n",
        "            else:\n",
        "                result = result.squeeze(1)\n",
        "            alpha = np.transpose(result, [1, 2, 0])\n",
        "\n",
        "        # background replace\n",
        "        h, w, _ = img.shape\n",
        "        if bg is None:\n",
        "            bg = np.ones_like(img)*255\n",
        "        else:\n",
        "            bg = cv2.resize(bg, (w, h))\n",
        "            if bg.ndim == 2:\n",
        "                bg = bg[..., np.newaxis]\n",
        "\n",
        "        comb = (alpha * img + (1 - alpha) * bg).astype(np.uint8)\n",
        "        return comb, alpha, bg, img"
      ],
      "metadata": {
        "id": "ndzz_HgmTI_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def angle_between_points(p0,p1,p2):\n",
        "    if p0[1]==-1 or p1[1]==-1 or p2[1]==-1:\n",
        "        return -1\n",
        "    a = (p1[0]-p0[0])**2 + (p1[1]-p0[1])**2\n",
        "    b = (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2\n",
        "    c = (p2[0]-p0[0])**2 + (p2[1]-p0[1])**2\n",
        "    if a * b == 0:\n",
        "        return -1\n",
        "    return math.acos((a+b-c) / math.sqrt(4*a*b)) * 180 / math.pi\n",
        "\n",
        "\n",
        "def crop_img_with_padding(img, keypoints, rect):\n",
        "    person_xmin,person_xmax, ymin, ymax= rect\n",
        "    img_h,img_w,_ = img.shape    ## find body center using keypoints\n",
        "    middle_shoulder_x = keypoints[1][0]\n",
        "    middle_hip_x = (keypoints[8][0] + keypoints[11][0]) // 2\n",
        "    mid_x = (middle_hip_x + middle_shoulder_x) // 2\n",
        "    mid_y = (ymin + ymax) // 2\n",
        "    ## find which side (l or r) is further than center x, use the further side\n",
        "    if abs(mid_x-person_xmin) > abs(person_xmax-mid_x): #left further\n",
        "        xmin = person_xmin\n",
        "        xmax = mid_x + (mid_x-person_xmin)\n",
        "    else:\n",
        "        ############### may be negtive\n",
        "        ### in this case, the script won't output any image, leave the case like this\n",
        "        ### since we don't want to pad human body\n",
        "        xmin = mid_x - (person_xmax-mid_x)\n",
        "        xmax = person_xmax\n",
        "\n",
        "    w = xmax - xmin\n",
        "    h = ymax - ymin\n",
        "    ## pad rectangle to w:h = 1:2 ## calculate desired border length\n",
        "    if h / w >= 2: #pad horizontally\n",
        "        target_w = h // 2\n",
        "        xmin_prime = int(mid_x - target_w / 2)\n",
        "        xmax_prime = int(mid_x + target_w / 2)\n",
        "        if xmin_prime < 0:\n",
        "            pad_left = abs(xmin_prime)# - xmin\n",
        "            xmin = 0\n",
        "        else:\n",
        "            pad_left = 0\n",
        "            xmin = xmin_prime\n",
        "        if xmax_prime > img_w:\n",
        "            pad_right = xmax_prime - img_w\n",
        "            xmax = img_w\n",
        "        else:\n",
        "            pad_right = 0\n",
        "            xmax = xmax_prime\n",
        "\n",
        "        cropped_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
        "        im_pad = cv2.copyMakeBorder(cropped_img, 0, 0, int(pad_left),  int(pad_right), cv2.BORDER_REPLICATE)\n",
        "    else: #pad vertically\n",
        "        target_h = w * 2\n",
        "        ymin_prime = mid_y - (target_h / 2)\n",
        "        ymax_prime = mid_y + (target_h / 2)\n",
        "        if ymin_prime < 0:\n",
        "            pad_up = abs(ymin_prime)# - ymin\n",
        "            ymin = 0\n",
        "        else:\n",
        "            pad_up = 0\n",
        "            ymin = ymin_prime\n",
        "        if ymax_prime > img_h:\n",
        "            pad_down = ymax_prime - img_h\n",
        "            ymax = img_h\n",
        "        else:\n",
        "            pad_down = 0\n",
        "            ymax = ymax_prime\n",
        "        print(ymin,ymax, xmin,xmax, img.shape)\n",
        "\n",
        "        cropped_img = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
        "        im_pad = cv2.copyMakeBorder(cropped_img, int(pad_up), int(pad_down), 0,\n",
        "                                    0, cv2.BORDER_REPLICATE)\n",
        "    result = cv2.resize(im_pad,(512,1024),interpolation = cv2.INTER_AREA)\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_alignment(image_folder):\n",
        "    dataset = ImagesDataset(image_folder, transforms.Compose([transforms.ToTensor()]))\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    body_estimation = Body('openpose/model/body_pose_model.pth')\n",
        "\n",
        "    total = len(dataloader)\n",
        "    print('Num of dataloader : ', total)\n",
        "\n",
        "    ## initialzide HumenSeg\n",
        "    human_seg_args = {}\n",
        "    human_seg_args['cfg'] = 'AvatarGen_Part1/deploy.yaml'\n",
        "    human_seg_args['input_shape'] = [1024,512]\n",
        "    human_seg_args['soft_predict'] = False\n",
        "    human_seg_args['use_gpu'] = True\n",
        "    human_seg_args['test_speed'] = False\n",
        "    human_seg_args['use_optic_flow'] = False\n",
        "    human_seg_args['add_argmax'] = True\n",
        "    human_seg_args= argparse.Namespace(**human_seg_args)\n",
        "    human_seg = PP_HumenSeg_Predictor(human_seg_args)\n",
        "\n",
        "    for fname, image in dataloader:\n",
        "        fname = fname[0]\n",
        "        print(f'Processing \\'{fname}\\'.')\n",
        "\n",
        "        image = (image.permute(0, 2, 3, 1) * 255).clamp(0, 255)\n",
        "        image = image.squeeze(0).numpy() # --> tensor to numpy, (H,W,C)\n",
        "        # avoid super high res img\n",
        "        if image.shape[0] >= 2000: # height  ### for shein image\n",
        "            ratio = image.shape[0]/1200 #height\n",
        "            dim = (int(image.shape[1]/ratio),1200)#(width, height)\n",
        "            image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        ## create segmentation\n",
        "        comb, segmentation, bg, ori_img = human_seg.run(image,None)\n",
        "\n",
        "        masks_np = (segmentation* 255)# .byte().cpu().numpy() #1024,512,1\n",
        "        mask0_np = masks_np[:,:,0].astype(np.uint8)#[0, :, :]\n",
        "        contours = cv2.findContours(mask0_np,  cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(contours)\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "        extBot = list(extBot)\n",
        "        extTop = list(extTop)\n",
        "        pad_range = int((extBot[1]-extTop[1])*0.05)\n",
        "        if (int(extTop[1])<=5 and int(extTop[1])>0) and (comb.shape[0]>int(extBot[1]) and int(extBot[1])>=comb.shape[0]-5): #seg mask already reaches to the edge\n",
        "            #pad with pure white, top 100 px, bottom 100 px\n",
        "            comb= cv2.copyMakeBorder(comb,pad_range+5,pad_range+5,0,0,cv2.BORDER_CONSTANT,value=[255,255,255])\n",
        "        elif int(extTop[1])<=0 or int(extBot[1])>=comb.shape[0]:\n",
        "            print('PAD: body out of boundary', fname) #should not happened\n",
        "            return {}\n",
        "        else:\n",
        "            comb = cv2.copyMakeBorder(comb, pad_range+5, pad_range+5, 0, 0, cv2.BORDER_REPLICATE) #105 instead of 100: give some extra space\n",
        "        extBot[1] = extBot[1] + pad_range+5\n",
        "        extTop[1] = extTop[1] + pad_range+5\n",
        "\n",
        "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "        extLeft = list(extLeft)\n",
        "        extRight = list(extRight)\n",
        "        person_ymin = int(extTop[1])-pad_range # 100\n",
        "        person_ymax = int(extBot[1])+pad_range # 100 #height\n",
        "        if person_ymin<0 or person_ymax>comb.shape[0]: # out of range\n",
        "            return {}\n",
        "        person_xmin = int(extLeft[0])\n",
        "        person_xmax = int(extRight[0])\n",
        "        rect =  [person_xmin,person_xmax,person_ymin, person_ymax]\n",
        "\n",
        "        ## detect keypoints\n",
        "        keypoints, subset = body_estimation(comb)\n",
        "        # print(keypoints, subset, len(subset))\n",
        "        if len(subset) != 1 or (len(subset)==1 and subset[0][-1]<15):\n",
        "            print(f'Processing \\'{fname}\\'. Please import image contains one person only. Also can check segmentation mask. ')\n",
        "            continue\n",
        "\n",
        "        comb = crop_img_with_padding(comb, keypoints, rect)\n",
        "\n",
        "        return comb"
      ],
      "metadata": {
        "id": "qlGr0FPRTa-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Detections is actually taken from PARE and is creating bboxes around a human in a picture.\n",
        "It is already loaded here so we can rotate the image accordingly after alignment"
      ],
      "metadata": {
        "id": "Him8U4obWVLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_detections(img_path, size=608):\n",
        "  detector = YOLOv3(device='cuda', img_size=size, person_detector=True, return_dict=True)\n",
        "  input_image = cv2.imread(img_path)\n",
        "  image_data = to_tensor(cv2.cvtColor(cv2.resize(input_image, (size, size)), cv2.COLOR_BGR2RGB))\n",
        "  dataloader = torch.utils.data.DataLoader([image_data], batch_size=12, num_workers=0)\n",
        "  device = torch.device('cuda')\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(device)\n",
        "    predictions = detector(batch)\n",
        "\n",
        "  detections = []\n",
        "  for pred in predictions:\n",
        "    bb = pred['boxes'].cpu().numpy()\n",
        "    sc = pred['scores'].cpu().numpy()[..., None]\n",
        "    dets = np.hstack([bb, sc])\n",
        "    dets = dets[sc[:, 0] > 0.7]\n",
        "    detections.append(dets)\n",
        "\n",
        "  new_detections = []\n",
        "  for frame_idx, dets in enumerate(detections):\n",
        "    img_dets = []\n",
        "    for d in dets:\n",
        "      w, h = d[2] - d[0], d[3] - d[1]\n",
        "      c_x, c_y = d[0] + w / 2, d[1] + h / 2\n",
        "      w = h = np.where(w / h > 1, w, h)\n",
        "      bbox = np.array([c_x, c_y, w, h])\n",
        "      img_dets.append(bbox)\n",
        "    new_detections.append(np.array(img_dets))\n",
        "\n",
        "    return new_detections"
      ],
      "metadata": {
        "id": "pR1ysw-TrTy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### actual usage"
      ],
      "metadata": {
        "id": "svbVSLWrTr4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alignment as well as other modules is loading the input image from a folder, so we create a new folder, and copy the image there"
      ],
      "metadata": {
        "id": "IoHmC8_zWob8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pictures"
      ],
      "metadata": {
        "id": "zglXcjirUk7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $image_file_path ./pictures"
      ],
      "metadata": {
        "id": "sfyB_4YoUnfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "\n",
        "### run the actual alignemnt\n",
        "aligned_image = run_alignment('./pictures')"
      ],
      "metadata": {
        "id": "8WWOIKvBTvXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7e3980-4233-49f0-fdad-2d7606266664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of dataloader :  1\n",
            "Processing 'scholz'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### check detection (also used later in PARE) to find out if picture needs to be rotated"
      ],
      "metadata": {
        "id": "Y_sbhSnNikxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_alignet_image():\n",
        "  image_to_save = Image.fromarray(aligned_image)\n",
        "  save_file_path = f'./pictures/{image_name}{image_type}'\n",
        "  image_to_save.save(save_file_path)\n",
        "  return save_file_path"
      ],
      "metadata": {
        "id": "QEGXm6uYcpQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file_path = save_alignet_image()\n",
        "dets = get_detections(save_file_path)\n",
        "while len(dets[0])==0:\n",
        "  aligned_image = np.rot90(aligned_image, k=-1)\n",
        "  dets = get_detections(save_file_path)\n",
        "save_alignet_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXRUd0Svi2u9",
        "outputId": "d873d16b-5e3e-4ff0-8cc3-d13f42f4be6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from https://pjreddie.com/media/files/yolov3.weights\n",
            "Downloading files from https://raw.githubusercontent.com/mkocabas/yolov3-pytorch/master/yolov3/config/yolov3.cfg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./pictures/scholz.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##show the aligned picture if needed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.imshow(aligned_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzD2Xs27-iYj",
        "outputId": "72eb4d60-1856-45d7-f014-02b4feebd729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create stylegan latent code"
      ],
      "metadata": {
        "id": "jUBt5lRHUpyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StyleGan latent code creation uses PTI as it is implemenmted in StyleHUman, can be found here: https://github.com/stylegan-human/StyleGAN-Human/blob/main/run_pti.py"
      ],
      "metadata": {
        "id": "ojMvGX8PXPtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "tvMZ1i4FVsRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils is creating all needed helper fuctions and classses so it can be run within the notebook"
      ],
      "metadata": {
        "id": "iIGptzChXb80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pti_global_config:\n",
        "  ## Device\n",
        "  cuda_visible_devices = '0'\n",
        "  device = 'cuda:0'\n",
        "\n",
        "  ## Logs\n",
        "  training_step = 1\n",
        "  image_rec_result_log_snapshot = 100\n",
        "  pivotal_training_steps = 0\n",
        "  model_snapshot_interval = 400\n",
        "\n",
        "  ## Run name to be updated during PTI\n",
        "  run_name = 'exp'\n",
        "\n",
        "class pti_path_config: # ToDo: Check if all is needed\n",
        "  stylegan2_ada_shhq = '/content/AvatarGen_Part1/AvatarGen_Part1/stylegan_human_v2_1024.pkl'\n",
        "  e4e = '/content/AvatarGen_Part1/AvatarGen_Part1/e4e_w+.pt'\n",
        "  checkpoints_dir = './output/pti/checkpoints/'\n",
        "  pti_results_keyword = 'PTI'\n",
        "  e4e_results_keyword = 'e4e'\n",
        "  embedding_base_dir = './output/pti/embeddings'\n",
        "  input_data_id = image_name\n",
        "  experiments_output_dir = './output/pti/'\n",
        "  input_data_path = './pictures'\n",
        "\n",
        "class pti_hyperparameters:\n",
        "  ## Architechture\n",
        "  lpips_type = 'alex'\n",
        "  first_inv_type = 'w+'\n",
        "  optim_type = 'adam'\n",
        "\n",
        "  ## Locality regularization\n",
        "  latent_ball_num_of_samples = 1\n",
        "  locality_regularization_interval = 1\n",
        "  use_locality_regularization = False\n",
        "  regulizer_l2_lambda = 0.1\n",
        "  regulizer_lpips_lambda = 0.1\n",
        "  regulizer_alpha = 30\n",
        "\n",
        "  ## Loss\n",
        "  pt_l2_lambda = 1\n",
        "  pt_lpips_lambda = 1\n",
        "\n",
        "  ## Steps\n",
        "  LPIPS_value_threshold = 0.04\n",
        "  max_pti_steps = 350\n",
        "  first_inv_steps = 450\n",
        "  max_images_to_invert = 30\n",
        "\n",
        "  ## Optimization\n",
        "  pti_learning_rate = 5e-4\n",
        "  first_inv_lr = 8e-3\n",
        "  train_batch_size = 1\n",
        "  use_last_w_pivots = False"
      ],
      "metadata": {
        "id": "3Op5NVD8ocPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_criterion = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "\n",
        "def l2_loss(real_images, generated_images):\n",
        "    loss = l2_criterion(real_images, generated_images)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class Space_Regulizer:\n",
        "    def __init__(self, original_G, lpips_net):\n",
        "        self.original_G = original_G\n",
        "        self.morphing_regulizer_alpha = pti_hyperparameters.regulizer_alpha\n",
        "        self.lpips_loss = lpips_net\n",
        "\n",
        "    def get_morphed_w_code(self, new_w_code, fixed_w):\n",
        "        interpolation_direction = new_w_code - fixed_w\n",
        "        interpolation_direction_norm = torch.norm(interpolation_direction, p=2)\n",
        "        direction_to_move = pti_hyperparameters.regulizer_alpha * interpolation_direction / interpolation_direction_norm\n",
        "        result_w = fixed_w + direction_to_move\n",
        "        self.morphing_regulizer_alpha * fixed_w + (1 - self.morphing_regulizer_alpha) * new_w_code\n",
        "\n",
        "        return result_w\n",
        "\n",
        "    def get_image_from_ws(self, w_codes, G):\n",
        "        return torch.cat([G.synthesis(w_code, noise_mode='none', force_fp32=True) for w_code in w_codes])\n",
        "\n",
        "    def ball_holder_loss_lazy(self, new_G, num_of_sampled_latents, w_batch, use_wandb=False):\n",
        "        loss = 0.0\n",
        "\n",
        "        z_samples = np.random.randn(num_of_sampled_latents, self.original_G.z_dim)\n",
        "        w_samples = self.original_G.mapping(torch.from_numpy(z_samples).to(pti_global_config.device), None,\n",
        "                                            truncation_psi=0.5)\n",
        "        territory_indicator_ws = [self.get_morphed_w_code(w_code.unsqueeze(0), w_batch) for w_code in w_samples]\n",
        "\n",
        "        for w_code in territory_indicator_ws:\n",
        "            new_img = new_G.synthesis(w_code, noise_mode='none', force_fp32=True)\n",
        "            with torch.no_grad():\n",
        "                old_img = self.original_G.synthesis(w_code, noise_mode='none', force_fp32=True)\n",
        "\n",
        "            if pti_hyperparameters.regulizer_l2_lambda > 0:\n",
        "                l2_loss_val = l2_loss.l2_loss(old_img, new_img)\n",
        "                loss += l2_loss_val * pti_hyperparameters.regulizer_l2_lambda\n",
        "\n",
        "            if pti_hyperparameters.regulizer_lpips_lambda > 0:\n",
        "                loss_lpips = self.lpips_loss(old_img, new_img)\n",
        "                loss_lpips = torch.mean(torch.squeeze(loss_lpips))\n",
        "                loss += loss_lpips * pti_hyperparameters.regulizer_lpips_lambda\n",
        "\n",
        "        return loss / len(territory_indicator_ws)\n",
        "\n",
        "    def space_regulizer_loss(self, new_G, w_batch, use_wandb):\n",
        "        ret_val = self.ball_holder_loss_lazy(new_G, pti_hyperparameters.latent_ball_num_of_samples, w_batch, use_wandb)\n",
        "        return ret_val"
      ],
      "metadata": {
        "id": "LbEPVkJ84MJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project(\n",
        "        G,\n",
        "        target: torch.Tensor,  # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "        *,\n",
        "        num_steps=1000,\n",
        "        w_avg_samples=10000,\n",
        "        initial_learning_rate=0.01,\n",
        "        initial_noise_factor=0.05,\n",
        "        lr_rampdown_length=0.25,\n",
        "        lr_rampup_length=0.05,\n",
        "        noise_ramp_length=0.75,\n",
        "        regularize_noise_weight=1e5,\n",
        "        verbose=False,\n",
        "        device: torch.device,\n",
        "        use_wandb=False,\n",
        "        initial_w=None,\n",
        "        image_log_step=pti_global_config.image_rec_result_log_snapshot,\n",
        "        w_name: str\n",
        "):\n",
        "    print(target.shape,G.img_channels, G.img_resolution, G.img_resolution//2)\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution // 2)\n",
        "\n",
        "    def logprint(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device).float()  # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)  # [N, 1, C]\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)  # [1, 1, C]\n",
        "    w_avg_tensor = torch.from_numpy(w_avg).to(pti_global_config.device)\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    start_w = initial_w if initial_w is not None else w_avg\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = {name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name}\n",
        "\n",
        "    # Load VGG16 feature detector.\n",
        "    model_file = 'AvatarGen_Part1/vgg16.pt'\n",
        "    with open(model_file, 'rb') as f:\n",
        "        vgg16 = torch.jit.load(f).eval().to(device)\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(start_w, dtype=torch.float32, device=device,\n",
        "                         requires_grad=True)  # pylint: disable=not-callable\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999),\n",
        "                                 lr=pti_hyperparameters.first_inv_lr)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth images from opt_w.\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255 / 2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "        dist = (target_features - synth_features).square().sum()\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None, None, :, :]  # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise * torch.roll(noise, shifts=1, dims=3)).mean() ** 2\n",
        "                reg_loss += (noise * torch.roll(noise, shifts=1, dims=2)).mean() ** 2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        loss = dist + reg_loss * regularize_noise_weight\n",
        "        if step % 10 == 0:\n",
        "            print(\"project loss\", step, loss.data)\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        logprint(f'step {step + 1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    del G\n",
        "    return w_opt.repeat([1, 18, 1])"
      ],
      "metadata": {
        "id": "UF0qYC7F56I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toogle_grad(model, flag=True):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "def load_old_G():\n",
        "    with open(pti_path_config.stylegan2_ada_shhq, 'rb') as f:\n",
        "        old_G = pickle.load(f)['G_ema'].to(pti_global_config.device).eval()\n",
        "        old_G = old_G.float()\n",
        "    return old_G"
      ],
      "metadata": {
        "id": "R7W2F71G9K2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pti_models.e4e.psp import pSp\n",
        "\n",
        "class BaseCoach:\n",
        "    def __init__(self, data_loader, use_wandb):\n",
        "\n",
        "        self.use_wandb = use_wandb\n",
        "        self.data_loader = data_loader\n",
        "        self.w_pivots = {}\n",
        "        self.image_counter = 0\n",
        "\n",
        "        if pti_hyperparameters.first_inv_type == 'w+':\n",
        "            self.initilize_e4e()\n",
        "\n",
        "        self.e4e_image_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((256, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "        # Initialize loss\n",
        "        self.lpips_loss = LPIPS(net=pti_hyperparameters.lpips_type).to(pti_global_config.device).eval()\n",
        "\n",
        "        self.restart_training()\n",
        "\n",
        "        # Initialize checkpoint dir\n",
        "        self.checkpoint_dir = pti_path_config.checkpoints_dir\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    def restart_training(self):\n",
        "\n",
        "        # Initialize networks\n",
        "        self.G = load_old_G()\n",
        "        toogle_grad(self.G, True)\n",
        "\n",
        "        self.original_G = load_old_G()\n",
        "\n",
        "        self.space_regulizer = Space_Regulizer(self.original_G, self.lpips_loss)\n",
        "        self.optimizer = self.configure_optimizers()\n",
        "\n",
        "    def get_inversion(self, w_path_dir, image_name, image):\n",
        "        embedding_dir = f'{w_path_dir}/{pti_path_config.pti_results_keyword}/{image_name}'\n",
        "        os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "        w_pivot = None\n",
        "\n",
        "        if pti_hyperparameters.use_last_w_pivots:\n",
        "            w_pivot = self.load_inversions(w_path_dir, image_name)\n",
        "\n",
        "        if not pti_hyperparameters.use_last_w_pivots or w_pivot is None:\n",
        "            w_pivot = self.calc_inversions(image, image_name)\n",
        "            torch.save(w_pivot, f'{embedding_dir}/0.pt')\n",
        "\n",
        "        w_pivot = w_pivot.to(pti_global_config.device)\n",
        "        return w_pivot\n",
        "\n",
        "    def load_inversions(self, w_path_dir, image_name):\n",
        "        if image_name in self.w_pivots:\n",
        "            return self.w_pivots[image_name]\n",
        "\n",
        "        if pti_hyperparameters.first_inv_type == 'w+':\n",
        "            w_potential_path = f'{w_path_dir}/{pti_path_config.e4e_results_keyword}/{image_name}/0.pt'\n",
        "        else:\n",
        "            w_potential_path = f'{w_path_dir}/{pti_path_config.pti_results_keyword}/{image_name}/0.pt'\n",
        "        if not os.path.isfile(w_potential_path):\n",
        "            return None\n",
        "        w = torch.load(w_potential_path).to(pti_global_config.device)\n",
        "        self.w_pivots[image_name] = w\n",
        "        return w\n",
        "\n",
        "    def calc_inversions(self, image, image_name):\n",
        "        if pti_hyperparameters.first_inv_type == 'w+':\n",
        "            w = self.get_e4e_inversion(image)\n",
        "\n",
        "        else:\n",
        "            id_image = torch.squeeze((image.to(pti_global_config.device) + 1) / 2) * 255\n",
        "            w = w_projector.project(self.G, id_image, device=torch.device(pti_global_config.device), w_avg_samples=600,\n",
        "                                    num_steps=pti_hyperparameters.first_inv_steps, w_name=image_name,\n",
        "                                    use_wandb=self.use_wandb)\n",
        "\n",
        "        return w\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.G.parameters(), lr=pti_hyperparameters.pti_learning_rate)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def calc_loss(self, generated_images, real_images, log_name, new_G, use_ball_holder, w_batch):\n",
        "        loss = 0.0\n",
        "\n",
        "        if pti_hyperparameters.pt_l2_lambda > 0:\n",
        "            l2_loss_val = l2_loss(generated_images, real_images)\n",
        "            loss += l2_loss_val * pti_hyperparameters.pt_l2_lambda\n",
        "        if pti_hyperparameters.pt_lpips_lambda > 0:\n",
        "            loss_lpips = self.lpips_loss(generated_images, real_images)\n",
        "            loss_lpips = torch.squeeze(loss_lpips)\n",
        "            loss += loss_lpips * pti_hyperparameters.pt_lpips_lambda\n",
        "\n",
        "        if use_ball_holder and pti_hyperparameters.use_locality_regularization:\n",
        "            ball_holder_loss_val = self.space_regulizer.space_regulizer_loss(new_G, w_batch, use_wandb=self.use_wandb)\n",
        "            loss += ball_holder_loss_val\n",
        "\n",
        "        return loss, l2_loss_val, loss_lpips\n",
        "\n",
        "    def forward(self, w):\n",
        "        generated_images = self.G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "        return generated_images\n",
        "\n",
        "    def initilize_e4e(self):\n",
        "        ckpt = torch.load(pti_path_config.e4e, map_location='cpu')\n",
        "        opts = ckpt['opts']\n",
        "        opts['batch_size'] = pti_hyperparameters.train_batch_size\n",
        "        opts['checkpoint_path'] = pti_path_config.e4e\n",
        "        opts = Namespace(**opts)\n",
        "        self.e4e_inversion_net = pSp(opts)\n",
        "        self.e4e_inversion_net.eval()\n",
        "        self.e4e_inversion_net = self.e4e_inversion_net.to(pti_global_config.device)\n",
        "        toogle_grad(self.e4e_inversion_net, False)\n",
        "\n",
        "    def get_e4e_inversion(self, image):\n",
        "        image = (image + 1) / 2\n",
        "        new_image = self.e4e_image_transform(image[0]).to(pti_global_config.device)\n",
        "        _, w = self.e4e_inversion_net(new_image.unsqueeze(0), randomize_noise=False, return_latents=True, resize=False,\n",
        "                                      input_code=False)\n",
        "        return w"
      ],
      "metadata": {
        "id": "spYjFMOB9oHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleIDCoach(BaseCoach):\n",
        "\n",
        "    def __init__(self, data_loader, use_wandb):\n",
        "        super().__init__(data_loader, use_wandb)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        w_path_dir = f'{pti_path_config.embedding_base_dir}/{pti_path_config.input_data_id}'\n",
        "        os.makedirs(w_path_dir, exist_ok=True)\n",
        "        os.makedirs(f'{w_path_dir}/{pti_path_config.pti_results_keyword}', exist_ok=True)\n",
        "\n",
        "        use_ball_holder = True\n",
        "\n",
        "        for fname, image in self.data_loader:\n",
        "            image_name = fname[0]\n",
        "\n",
        "            self.restart_training()\n",
        "\n",
        "            if self.image_counter >= pti_hyperparameters.max_images_to_invert:\n",
        "                break\n",
        "\n",
        "            embedding_dir = f'{w_path_dir}/{pti_path_config.pti_results_keyword}/{image_name}'\n",
        "            os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "            w_pivot = None\n",
        "\n",
        "            if pti_hyperparameters.use_last_w_pivots:\n",
        "                w_pivot = self.load_inversions(w_path_dir, image_name)\n",
        "\n",
        "            elif not pti_hyperparameters.use_last_w_pivots or w_pivot is None:\n",
        "                w_pivot = self.calc_inversions(image, image_name)\n",
        "\n",
        "            # w_pivot = w_pivot.detach().clone().to(global_config.device)\n",
        "            w_pivot = w_pivot.to(pti_global_config.device)\n",
        "\n",
        "            torch.save(w_pivot, f'{embedding_dir}/0.pt')\n",
        "            log_images_counter = 0\n",
        "            real_images_batch = image.to(pti_global_config.device)\n",
        "\n",
        "            for i in range(pti_hyperparameters.max_pti_steps):\n",
        "\n",
        "                generated_images = self.forward(w_pivot)\n",
        "                loss, l2_loss_val, loss_lpips = self.calc_loss(generated_images, real_images_batch, image_name,\n",
        "                                                               self.G, use_ball_holder, w_pivot)\n",
        "                if i == 0:\n",
        "                    tmp1 = torch.clone(generated_images)\n",
        "                if i % 50 == 0:\n",
        "                    print(\"pti loss: \", i, loss.data, loss_lpips.data)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if loss_lpips <= pti_hyperparameters.LPIPS_value_threshold:\n",
        "                    break\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                use_ball_holder = pti_global_config.training_step % pti_hyperparameters.locality_regularization_interval == 0\n",
        "\n",
        "                pti_global_config.training_step += 1\n",
        "                log_images_counter += 1\n",
        "\n",
        "            # save output image\n",
        "            tmp = torch.cat([real_images_batch, tmp1, generated_images], axis= 3)\n",
        "            save_image(tmp, f\"{pti_path_config.experiments_output_dir}/{image_name}.png\", normalize=True)\n",
        "\n",
        "\n",
        "            self.image_counter += 1\n",
        "\n",
        "            snapshot_data = dict()\n",
        "            snapshot_data['G_ema'] = self.G\n",
        "            import pickle\n",
        "            with open(f'{pti_path_config.checkpoints_dir}/model_{image_name}.pkl', 'wb') as f:\n",
        "                    pickle.dump(snapshot_data, f)"
      ],
      "metadata": {
        "id": "7wa7Vkg9-oEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_PTI(run_name=''):\n",
        "    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = pti_global_config.cuda_visible_devices\n",
        "\n",
        "    if run_name == '':\n",
        "        pti_global_config.run_name = ''.join(choice(ascii_uppercase) for i in range(12))\n",
        "    else:\n",
        "        pti_global_config.run_name = run_name\n",
        "\n",
        "    pti_global_config.pivotal_training_steps = 1\n",
        "    pti_global_config.training_step = 1\n",
        "\n",
        "    embedding_dir_path = f'output/pti/{image_name}'\n",
        "    os.makedirs(embedding_dir_path, exist_ok=True)\n",
        "\n",
        "    dataset = ImagesDataset(pti_path_config.input_data_path, transforms.Compose([\n",
        "        transforms.Resize((1024, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]))\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    coach = SingleIDCoach(dataloader, False)\n",
        "\n",
        "    coach.train()\n",
        "\n",
        "    return pti_global_config.run_name"
      ],
      "metadata": {
        "id": "VcSXiuiwV34_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### usage"
      ],
      "metadata": {
        "id": "y7aQbJPxYpV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual usage of PTI.\n",
        "run_pti just returns the name of the running process, and saves the data within a folder"
      ],
      "metadata": {
        "id": "e5DcXQurXqnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = run_PTI()"
      ],
      "metadata": {
        "id": "3_62fbGcYrxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e2c137-273d-414c-9e40-5757c60b2172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: /content/AvatarGen_Part1/AvatarGen_Part1/e4e_w+.pt\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "pti loss:  0 tensor(0.2967, device='cuda:0') tensor(0.2255, device='cuda:0')\n",
            "pti loss:  50 tensor(0.1461, device='cuda:0') tensor(0.1170, device='cuda:0')\n",
            "pti loss:  100 tensor(0.1081, device='cuda:0') tensor(0.0867, device='cuda:0')\n",
            "pti loss:  150 tensor(0.0893, device='cuda:0') tensor(0.0711, device='cuda:0')\n",
            "pti loss:  200 tensor(0.0770, device='cuda:0') tensor(0.0613, device='cuda:0')\n",
            "pti loss:  250 tensor(0.0664, device='cuda:0') tensor(0.0525, device='cuda:0')\n",
            "pti loss:  300 tensor(0.0580, device='cuda:0') tensor(0.0456, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### load latent code from file\n",
        "\n",
        "pti_file = os.getcwd() +  f'/output/pti/embeddings/{image_name}/PTI/{image_name}/0.pt'\n",
        "\n",
        "latent_code = torch.load(pti_file)"
      ],
      "metadata": {
        "id": "v298qEbgTXf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNARF"
      ],
      "metadata": {
        "id": "fwV2YC_pZ5-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using GNARF to get the triplane representation of the latent code, to sample features\n",
        "GNARF is a model able to create 3D Human representations, but we don't need all of it.\n",
        "In here we are creating the environment needed for Gnarf, load a pre-trained model but then adjust the synthesis function to only get the triplane result."
      ],
      "metadata": {
        "id": "8oCN6vjgYSY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download files for env"
      ],
      "metadata": {
        "id": "lhxJglbNmy8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir GNARF"
      ],
      "metadata": {
        "id": "T4A_Ng7MX2Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('GNARF')"
      ],
      "metadata": {
        "id": "SzOsANAoX6Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1z4s_Wfbh6v4AM2LanZlNhLNlS046_yJh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka07AYDwenlr",
        "outputId": "edf73ac6-6839-4831-d7bb-f0ec00625969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1z4s_Wfbh6v4AM2LanZlNhLNlS046_yJh\n",
            "From (redirected): https://drive.google.com/uc?id=1z4s_Wfbh6v4AM2LanZlNhLNlS046_yJh&confirm=t&uuid=74752ad2-9920-4f13-912d-d1acfca3a7b4\n",
            "To: /content/AvatarGen_Part1/GNARF/utils.zip\n",
            "100% 772M/772M [00:04<00:00, 191MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q utils\n",
        "!rm utils.zip"
      ],
      "metadata": {
        "id": "ttU2d0zCfIIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model and create triplane"
      ],
      "metadata": {
        "id": "odOm9bnAm8T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('utils')"
      ],
      "metadata": {
        "id": "zU4oUqCrhZq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### our adjusted snythesis function to only get triplane\n",
        "\n",
        "from training.networks_stylegan2 import Generator as StyleGAN2Backbone\n",
        "\n",
        "def plane_synthesis(ws, c, neural_rendering_resolution=64, update_emas=False, cache_backbone=False, use_cached_backbone=False, **synthesis_kwargs):\n",
        "\n",
        "        backbone = StyleGAN2Backbone(z_dim=512, c_dim=107, w_dim=512, img_resolution = 256, img_channels=96)\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        backbone = backbone.to(device)\n",
        "\n",
        "        cam2world_matrix = c[:, :16].view(-1, 4, 4)\n",
        "        intrinsics = c[:, 16:25].view(-1, 3, 3)\n",
        "\n",
        "        # Create triplanes by running StyleGAN backbone\n",
        "        planes = backbone.synthesis(ws, update_emas=update_emas)\n",
        "\n",
        "\n",
        "        # Reshape output into three 32-channel planes\n",
        "        planes = planes.view(len(planes), 3, 32, planes.shape[-2], planes.shape[-1])\n",
        "\n",
        "        return {'planes': planes}\n"
      ],
      "metadata": {
        "id": "qjkPsjvnNoal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### load pre-trained model\n",
        "\n",
        "with open('AvatarGen_Part1/network-snapshot-004800.pkl', 'rb') as file:\n",
        "  gnarf_pretrained = pickle.load(file)\n",
        "\n",
        "Generator = gnarf_pretrained.get('G_ema').cuda()"
      ],
      "metadata": {
        "id": "dQK0u-Z1Z9nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## camera for shhq dataset taken from implementation\n",
        "### https://github.com/alexanderbergman7/GNARF/blob/89d2bfed9bf9937931e7922bdd66d43cc1d0125b/generate_video.py#L40C3-L40C3\n",
        "\n",
        "camera = np.array([1., 0., 0., -0.0132909, 0., 1., 0., -0.10394844, 0., 0., 1., -10.351759, 0., 0., 0., 1., 9.784736, 0., 0.5, 0., 4.8875856, 0.5, 0., 0., 1., -3.0198941, 0.01279547, 0.40898454])\n",
        "camera[16] /= 2\n",
        "\n",
        "device = torch.device('cuda', 0)\n",
        "\n",
        "c = torch.zeros(18, 107, device=device)\n",
        "c[:, :28] = torch.from_numpy(camera).to(device)"
      ],
      "metadata": {
        "id": "JOEduKsxfb05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Generator = copy.deepcopy(Generator).eval().requires_grad_(False).to(device)"
      ],
      "metadata": {
        "id": "DMod5eCMjf35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_pti = latent_code.squeeze()"
      ],
      "metadata": {
        "id": "qB5QLf4Efyl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## get weitgghs from pre-trained model\n",
        "\n",
        "ws = Generator.mapping(z=z_pti, c=c)"
      ],
      "metadata": {
        "id": "Ij31EFvCO-Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## actually create planes\n",
        "\n",
        "planes = plane_synthesis(ws=ws, c=c)"
      ],
      "metadata": {
        "id": "8628IoonP9T9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39c7214-95df-4ffe-da4f-85582b87f9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triplane = planes.get('planes')"
      ],
      "metadata": {
        "id": "POS9UpnxWwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PARE\n",
        "\n"
      ],
      "metadata": {
        "id": "RbWlmpkgljV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARE is used to retrieve the pose of the human in the input image.\n",
        "Its implementation is taken from here: https://github.com/mkocabas/PARE/tree/master"
      ],
      "metadata": {
        "id": "2Tsnzu48ZjtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "6isy2F2pmb3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create all necesary helper functions and classes and create environment to actually run PARE within the notebook"
      ],
      "metadata": {
        "id": "drXQpvgaZ5oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.getcwd() == '/content/AvatarGen_Part1/GNARF/utils':\n",
        "  os.chdir('../..')\n",
        "elif not os.getcwd() == '/content/AvatarGen_Part1':\n",
        "  print(os.getcwd())\n",
        "  raise FileNotFoundError(\"Please check if you are in the right folder\")"
      ],
      "metadata": {
        "id": "ayEgR0_UXLwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PARE_models.PAREConfig import update_hparams\n",
        "from PARE_models.PARE_utils.utils.train_utils import load_pretrained_model # this is heavy\n",
        "\n",
        "from PARE_models.PARE_utils.backbone import *\n",
        "from PARE_models.PARE_utils.head import PareHead, SMPLHead, SMPLCamHead\n",
        "from PARE_models.PARE_utils.backbone.utils import get_backbone_info\n",
        "from PARE_models.PARE_utils.backbone.hrnet import hrnet_w32, hrnet_w48"
      ],
      "metadata": {
        "id": "XrdSK0fwlqkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07beaea1-af83-4653-ddbd-bf183b4b524a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/AvatarGen_Part1/PARE_models/PARE_utils/backbone/hrnet.py:553: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  or self.pretrained_layers[0] is '*':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PARE(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_joints=24,\n",
        "            softmax_temp=1.0,\n",
        "            num_features_smpl=64,\n",
        "            backbone='resnet50',\n",
        "            focal_length=5000.,\n",
        "            img_res=224,\n",
        "            pretrained=None,\n",
        "            iterative_regression=False,\n",
        "            iter_residual=False,\n",
        "            num_iterations=3,\n",
        "            shape_input_type='feats',  # 'feats.all_pose.shape.cam',\n",
        "            pose_input_type='feats', # 'feats.neighbor_pose_feats.all_pose.self_pose.neighbor_pose.shape.cam'\n",
        "            pose_mlp_num_layers=1,\n",
        "            shape_mlp_num_layers=1,\n",
        "            pose_mlp_hidden_size=256,\n",
        "            shape_mlp_hidden_size=256,\n",
        "            use_keypoint_features_for_smpl_regression=False,\n",
        "            use_heatmaps='',\n",
        "            use_keypoint_attention=False,\n",
        "            keypoint_attention_act='softmax',\n",
        "            use_postconv_keypoint_attention=False,\n",
        "            use_scale_keypoint_attention=False,\n",
        "            use_final_nonlocal=None,\n",
        "            use_branch_nonlocal=None,\n",
        "            use_hmr_regression=False,\n",
        "            use_coattention=False,\n",
        "            num_coattention_iter=1,\n",
        "            coattention_conv='simple',\n",
        "            deconv_conv_kernel_size=4,\n",
        "            use_upsampling=False,\n",
        "            use_soft_attention=False,\n",
        "            num_branch_iteration=0,\n",
        "            branch_deeper=False,\n",
        "            num_deconv_layers=3,\n",
        "            num_deconv_filters=256,\n",
        "            use_resnet_conv_hrnet=False,\n",
        "            use_position_encodings=None,\n",
        "            use_mean_camshape=False,\n",
        "            use_mean_pose=False,\n",
        "            init_xavier=False,\n",
        "            use_cam=False,\n",
        "    ):\n",
        "        super(PARE, self).__init__()\n",
        "        if backbone.startswith('hrnet'):\n",
        "            backbone, use_conv = backbone.split('-')\n",
        "            # hrnet_w32-conv, hrnet_w32-interp\n",
        "            self.backbone = eval(backbone)(\n",
        "                pretrained=True,\n",
        "                downsample=False,\n",
        "                use_conv=(use_conv == 'conv')\n",
        "            )\n",
        "        else:\n",
        "            self.backbone = eval(backbone)(pretrained=True)\n",
        "\n",
        "        # self.backbone = eval(backbone)(pretrained=True)\n",
        "        self.head = PareHead(\n",
        "            num_joints=num_joints,\n",
        "            num_input_features=get_backbone_info(backbone)['n_output_channels'],\n",
        "            softmax_temp=softmax_temp,\n",
        "            num_deconv_layers=num_deconv_layers,\n",
        "            num_deconv_filters=[num_deconv_filters] * num_deconv_layers,\n",
        "            num_deconv_kernels=[deconv_conv_kernel_size] * num_deconv_layers,\n",
        "            num_features_smpl=num_features_smpl,\n",
        "            final_conv_kernel=1,\n",
        "            iterative_regression=iterative_regression,\n",
        "            iter_residual=iter_residual,\n",
        "            num_iterations=num_iterations,\n",
        "            shape_input_type=shape_input_type,\n",
        "            pose_input_type=pose_input_type,\n",
        "            pose_mlp_num_layers=pose_mlp_num_layers,\n",
        "            shape_mlp_num_layers=shape_mlp_num_layers,\n",
        "            pose_mlp_hidden_size=pose_mlp_hidden_size,\n",
        "            shape_mlp_hidden_size=shape_mlp_hidden_size,\n",
        "            use_keypoint_features_for_smpl_regression=use_keypoint_features_for_smpl_regression,\n",
        "            use_heatmaps=use_heatmaps,\n",
        "            use_keypoint_attention=use_keypoint_attention,\n",
        "            use_postconv_keypoint_attention=use_postconv_keypoint_attention,\n",
        "            keypoint_attention_act=keypoint_attention_act,\n",
        "            use_scale_keypoint_attention=use_scale_keypoint_attention,\n",
        "            use_branch_nonlocal=use_branch_nonlocal, # 'concatenation', 'dot_product', 'embedded_gaussian', 'gaussian'\n",
        "            use_final_nonlocal=use_final_nonlocal, # 'concatenation', 'dot_product', 'embedded_gaussian', 'gaussian'\n",
        "            backbone=backbone,\n",
        "            use_hmr_regression=use_hmr_regression,\n",
        "            use_coattention=use_coattention,\n",
        "            num_coattention_iter=num_coattention_iter,\n",
        "            coattention_conv=coattention_conv,\n",
        "            use_upsampling=use_upsampling,\n",
        "            use_soft_attention=use_soft_attention,\n",
        "            num_branch_iteration=num_branch_iteration,\n",
        "            branch_deeper=branch_deeper,\n",
        "            use_resnet_conv_hrnet=use_resnet_conv_hrnet,\n",
        "            use_position_encodings=use_position_encodings,\n",
        "            use_mean_camshape=use_mean_camshape,\n",
        "            use_mean_pose=use_mean_pose,\n",
        "            init_xavier=init_xavier,\n",
        "        )\n",
        "\n",
        "        self.use_cam = use_cam\n",
        "        if self.use_cam:\n",
        "            self.smpl = SMPLCamHead(\n",
        "                img_res=img_res,\n",
        "            )\n",
        "        else:\n",
        "            self.smpl = SMPLHead(\n",
        "                focal_length=focal_length,\n",
        "                img_res=img_res\n",
        "            )\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.load_pretrained(pretrained)\n",
        "\n",
        "    def load_pretrained(self, file):\n",
        "        print(f'Loading pretrained weights from {file}')\n",
        "        state_dict = torch.load(file)\n",
        "        self.backbone.load_state_dict(state_dict, strict=False)\n",
        "        load_pretrained_model(self.head, state_dict=state_dict, strict=False, overwrite_shape_mismatch=True)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            images,\n",
        "            cam_rotmat=None,\n",
        "            cam_intrinsics=None,\n",
        "            bbox_scale=None,\n",
        "            bbox_center=None,\n",
        "            img_w=None,\n",
        "            img_h=None,\n",
        "            gt_segm=None,\n",
        "    ):\n",
        "        features = self.backbone(images)\n",
        "        hmr_output = self.head(features, gt_segm=gt_segm)\n",
        "\n",
        "        if self.use_cam:\n",
        "            smpl_output = self.smpl(\n",
        "                rotmat=hmr_output['pred_pose'],\n",
        "                shape=hmr_output['pred_shape'],\n",
        "                cam=hmr_output['pred_cam'],\n",
        "                cam_rotmat=cam_rotmat,\n",
        "                cam_intrinsics=cam_intrinsics,\n",
        "                bbox_scale=bbox_scale,\n",
        "                bbox_center=bbox_center,\n",
        "                img_w=img_w,\n",
        "                img_h=img_h,\n",
        "                normalize_joints2d=True,\n",
        "            )\n",
        "            smpl_output.update(hmr_output)\n",
        "        else:\n",
        "            if isinstance(hmr_output['pred_pose'], list):\n",
        "                # if we have multiple smpl params prediction\n",
        "                # create a dictionary of lists per prediction\n",
        "                smpl_output = {\n",
        "                    'smpl_vertices': [],\n",
        "                    'smpl_joints3d': [],\n",
        "                    'smpl_joints2d': [],\n",
        "                    'pred_cam_t': [],\n",
        "                }\n",
        "                for idx in range(len(hmr_output['pred_pose'])):\n",
        "                    smpl_out = self.smpl(\n",
        "                        rotmat=hmr_output['pred_pose'][idx],\n",
        "                        shape=hmr_output['pred_shape'][idx],\n",
        "                        cam=hmr_output['pred_cam'][idx],\n",
        "                        normalize_joints2d=True,\n",
        "                    )\n",
        "                    for k, v in smpl_out.items():\n",
        "                        smpl_output[k].append(v)\n",
        "            else:\n",
        "                smpl_output = self.smpl(\n",
        "                    rotmat=hmr_output['pred_pose'],\n",
        "                    shape=hmr_output['pred_shape'],\n",
        "                    cam=hmr_output['pred_cam'],\n",
        "                    normalize_joints2d=True,\n",
        "                )\n",
        "                smpl_output.update(hmr_output)\n",
        "        return smpl_output"
      ],
      "metadata": {
        "id": "2mhfISPHmd96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_patch_image_cv(cvimg, c_x, c_y, bb_width, bb_height, patch_width, patch_height, scale, rot):\n",
        "    img = cvimg.copy()\n",
        "\n",
        "    trans = gen_trans_from_patch_cv(c_x, c_y, bb_width, bb_height, patch_width, patch_height, scale, rot)\n",
        "\n",
        "    img_patch = cv2.warpAffine(img, trans, (int(patch_width), int(patch_height)),\n",
        "                               flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
        "    return img_patch\n",
        "\n",
        "def rotate_2d(pt_2d, rot_rad):\n",
        "    x = pt_2d[0]\n",
        "    y = pt_2d[1]\n",
        "    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n",
        "    xx = x * cs - y * sn\n",
        "    yy = x * sn + y * cs\n",
        "    return np.array([xx, yy], dtype=np.float32)\n",
        "\n",
        "def gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, scale, rot):\n",
        "    # augment size with scale\n",
        "    src_w = src_width * scale\n",
        "    src_h = src_height * scale\n",
        "    src_center = np.zeros(2)\n",
        "    src_center[0] = c_x\n",
        "    src_center[1] = c_y # np.array([c_x, c_y], dtype=np.float32)\n",
        "    # augment rotation\n",
        "    rot_rad = np.pi * rot / 180\n",
        "    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n",
        "    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n",
        "\n",
        "    dst_w = dst_width\n",
        "    dst_h = dst_height\n",
        "    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n",
        "    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n",
        "    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n",
        "\n",
        "    src = np.zeros((3, 2), dtype=np.float32)\n",
        "    src[0, :] = src_center\n",
        "    src[1, :] = src_center + src_downdir\n",
        "    src[2, :] = src_center + src_rightdir\n",
        "\n",
        "    dst = np.zeros((3, 2), dtype=np.float32)\n",
        "    dst[0, :] = dst_center\n",
        "    dst[1, :] = dst_center + dst_downdir\n",
        "    dst[2, :] = dst_center + dst_rightdir\n",
        "\n",
        "    trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n",
        "\n",
        "    return trans\n",
        "\n",
        "def get_single_image_crop(image, bbox, scale=1.2, crop_size=224):\n",
        "\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        raise ('Unknown type for object', type(image))\n",
        "\n",
        "    if len(bbox) == 1:\n",
        "      bbox = bbox[0]\n",
        "\n",
        "    crop_image = generate_patch_image_cv(\n",
        "        cvimg=image.copy(),\n",
        "        c_x=bbox[0],\n",
        "        c_y=bbox[1],\n",
        "        bb_width=bbox[2],\n",
        "        bb_height=bbox[3],\n",
        "        patch_width=crop_size,\n",
        "        patch_height=crop_size,\n",
        "        scale=scale,\n",
        "        rot=0,\n",
        "    )\n",
        "\n",
        "    raw_image = crop_image.copy()\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    crop_image = transform(crop_image)\n",
        "\n",
        "    return crop_image, raw_image\n",
        "\n"
      ],
      "metadata": {
        "id": "JzZ95v2Wmh5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_crop_coords_to_orig_img(bbox, keypoints, crop_size):\n",
        "\n",
        "    if len(bbox) == 1:\n",
        "      bbox = bbox[0]\n",
        "\n",
        "    # import IPython; IPython.embed(); exit()\n",
        "    cx, cy, h = bbox[0], bbox[1], bbox[2]\n",
        "\n",
        "    # unnormalize to crop coords\n",
        "    keypoints = 0.5 * crop_size * (keypoints + 1.0)\n",
        "\n",
        "    # rescale to orig img crop\n",
        "    keypoints *= h[..., None, None] / crop_size\n",
        "\n",
        "    # transform into original image coords\n",
        "    keypoints[:,:,0] = (cx - h/2)[..., None] + keypoints[:,:,0]\n",
        "    keypoints[:,:,1] = (cy - h/2)[..., None] + keypoints[:,:,1]\n",
        "    return keypoints\n",
        "\n",
        "\n",
        "def convert_crop_cam_to_orig_img(cam, bbox, img_width, img_height):\n",
        "    '''\n",
        "    Convert predicted camera from cropped image coordinates\n",
        "    to original image coordinates\n",
        "    :param cam (ndarray, shape=(3,)): weak perspective camera in cropped img coordinates\n",
        "    :param bbox (ndarray, shape=(4,)): bbox coordinates (c_x, c_y, h)\n",
        "    :param img_width (int): original image width\n",
        "    :param img_height (int): original image height\n",
        "    :return:\n",
        "    '''\n",
        "    if len(bbox) == 1:\n",
        "      bbox = bbox[0]\n",
        "\n",
        "    cx, cy, h = bbox[:,0], bbox[:,1], bbox[:,2]\n",
        "    hw, hh = img_width / 2., img_height / 2.\n",
        "    sx = cam[:,0] * (1. / (img_width / h))\n",
        "    sy = cam[:,0] * (1. / (img_height / h))\n",
        "    tx = ((cx - hw) / hw / sx) + cam[:,1]\n",
        "    ty = ((cy - hh) / hh / sy) + cam[:,2]\n",
        "    orig_cam = np.stack([sx, sy, tx, ty]).T\n",
        "    return orig_cam"
      ],
      "metadata": {
        "id": "X468fj8Hqfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARETester is our custom and rather lightweight implementation of PARE"
      ],
      "metadata": {
        "id": "2-0G3qiyaJz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PARETester:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.model_cfg = update_hparams(args.cfg)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        self.model = self._build_model()\n",
        "        self._load_pretrained_model()\n",
        "        self.model.eval()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # ========= Define PARE model ========= #\n",
        "        model_cfg = self.model_cfg\n",
        "\n",
        "        if model_cfg.METHOD == 'pare':\n",
        "            model = PARE(\n",
        "                backbone=model_cfg.PARE.BACKBONE,\n",
        "                num_joints=model_cfg.PARE.NUM_JOINTS,\n",
        "                softmax_temp=model_cfg.PARE.SOFTMAX_TEMP,\n",
        "                num_features_smpl=model_cfg.PARE.NUM_FEATURES_SMPL,\n",
        "                focal_length=model_cfg.DATASET.FOCAL_LENGTH,\n",
        "                img_res=model_cfg.DATASET.IMG_RES,\n",
        "                pretrained=model_cfg.TRAINING.PRETRAINED,\n",
        "                iterative_regression=model_cfg.PARE.ITERATIVE_REGRESSION,\n",
        "                num_iterations=model_cfg.PARE.NUM_ITERATIONS,\n",
        "                iter_residual=model_cfg.PARE.ITER_RESIDUAL,\n",
        "                shape_input_type=model_cfg.PARE.SHAPE_INPUT_TYPE,\n",
        "                pose_input_type=model_cfg.PARE.POSE_INPUT_TYPE,\n",
        "                pose_mlp_num_layers=model_cfg.PARE.POSE_MLP_NUM_LAYERS,\n",
        "                shape_mlp_num_layers=model_cfg.PARE.SHAPE_MLP_NUM_LAYERS,\n",
        "                pose_mlp_hidden_size=model_cfg.PARE.POSE_MLP_HIDDEN_SIZE,\n",
        "                shape_mlp_hidden_size=model_cfg.PARE.SHAPE_MLP_HIDDEN_SIZE,\n",
        "                use_keypoint_features_for_smpl_regression=model_cfg.PARE.USE_KEYPOINT_FEATURES_FOR_SMPL_REGRESSION,\n",
        "                use_heatmaps=model_cfg.DATASET.USE_HEATMAPS,\n",
        "                use_keypoint_attention=model_cfg.PARE.USE_KEYPOINT_ATTENTION,\n",
        "                use_postconv_keypoint_attention=model_cfg.PARE.USE_POSTCONV_KEYPOINT_ATTENTION,\n",
        "                use_scale_keypoint_attention=model_cfg.PARE.USE_SCALE_KEYPOINT_ATTENTION,\n",
        "                keypoint_attention_act=model_cfg.PARE.KEYPOINT_ATTENTION_ACT,\n",
        "                use_final_nonlocal=model_cfg.PARE.USE_FINAL_NONLOCAL,\n",
        "                use_branch_nonlocal=model_cfg.PARE.USE_BRANCH_NONLOCAL,\n",
        "                use_hmr_regression=model_cfg.PARE.USE_HMR_REGRESSION,\n",
        "                use_coattention=model_cfg.PARE.USE_COATTENTION,\n",
        "                num_coattention_iter=model_cfg.PARE.NUM_COATTENTION_ITER,\n",
        "                coattention_conv=model_cfg.PARE.COATTENTION_CONV,\n",
        "                use_upsampling=model_cfg.PARE.USE_UPSAMPLING,\n",
        "                deconv_conv_kernel_size=model_cfg.PARE.DECONV_CONV_KERNEL_SIZE,\n",
        "                use_soft_attention=model_cfg.PARE.USE_SOFT_ATTENTION,\n",
        "                num_branch_iteration=model_cfg.PARE.NUM_BRANCH_ITERATION,\n",
        "                branch_deeper=model_cfg.PARE.BRANCH_DEEPER,\n",
        "                num_deconv_layers=model_cfg.PARE.NUM_DECONV_LAYERS,\n",
        "                num_deconv_filters=model_cfg.PARE.NUM_DECONV_FILTERS,\n",
        "                use_resnet_conv_hrnet=model_cfg.PARE.USE_RESNET_CONV_HRNET,\n",
        "                use_position_encodings=model_cfg.PARE.USE_POS_ENC,\n",
        "                use_mean_camshape=model_cfg.PARE.USE_MEAN_CAMSHAPE,\n",
        "                use_mean_pose=model_cfg.PARE.USE_MEAN_POSE,\n",
        "                init_xavier=model_cfg.PARE.INIT_XAVIER,\n",
        "            ).to(self.device)\n",
        "        else:\n",
        "            print(f'{model_cfg.METHOD} is undefined!')\n",
        "            exit()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _load_pretrained_model(self):\n",
        "        # ========= Load pretrained weights ========= #\n",
        "        print(f'Loading pretrained model from {self.args.ckpt}')\n",
        "        ckpt = torch.load(self.args.ckpt)['state_dict']\n",
        "        load_pretrained_model(self.model, ckpt, overwrite_shape_mismatch=True, remove_lightning=True)\n",
        "        (f'Loaded pretrained weights from \\\"{self.args.ckpt}\\\"')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def run_on_image(self, image, detections):\n",
        "        joints2d = None\n",
        "        dets = detections[0]\n",
        "        det_idx = 0\n",
        "        img = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n",
        "        orig_height, orig_width = img.shape[:2]\n",
        "        inp_images = torch.zeros(len(dets), 3, self.model_cfg.DATASET.IMG_RES,\n",
        "                                 self.model_cfg.DATASET.IMG_RES, device=self.device, dtype=torch.float)\n",
        "        norm_joints2d = []\n",
        "        bbox = dets\n",
        "        norm_img, raw_img = get_single_image_crop(\n",
        "                    img,\n",
        "                    bbox,\n",
        "                    scale=1.0,\n",
        "                    crop_size=self.model_cfg.DATASET.IMG_RES\n",
        "                )\n",
        "\n",
        "        inp_images[det_idx] = norm_img.float().to(self.device)\n",
        "\n",
        "        output = self.model(inp_images)\n",
        "        for k, v in output.items():\n",
        "          output[k] = v.cpu().numpy()\n",
        "\n",
        "\n",
        "        smpl_joints2d = convert_crop_coords_to_orig_img(\n",
        "          bbox=dets,\n",
        "          keypoints=output['smpl_joints2d'],\n",
        "          crop_size=self.model_cfg.DATASET.IMG_RES,\n",
        "        )\n",
        "\n",
        "        output['bboxes'] = dets\n",
        "        output['smpl_joints2d'] = smpl_joints2d\n",
        "\n",
        "        orig_cam = convert_crop_cam_to_orig_img(\n",
        "                cam=output['pred_cam'],\n",
        "                bbox=[dets],\n",
        "                img_width=orig_width,\n",
        "                img_height=orig_height\n",
        "            )\n",
        "\n",
        "        output['origcam'] = orig_cam\n",
        "\n",
        "\n",
        "        del inp_images\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "nE44v5rEqqBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### actual usage"
      ],
      "metadata": {
        "id": "JhthmWX-rmsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.chdir('PARE_models')\n",
        "except:\n",
        "  print(os.getcwd())"
      ],
      "metadata": {
        "id": "p2I32c2ydygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### download pre-trained models and configuration files into the PARE folder\n",
        "\n",
        "!gdown -q --folder https://drive.google.com/drive/folders/1PWmAzDg6v33kyLbG1TPEAeR7x4JGuJ08?usp=drive_link"
      ],
      "metadata": {
        "id": "qAU4mRWTrz18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### custom starter class using the pre-defined\n",
        "\n",
        "class PAREStarter:\n",
        "  cfg ='data/pare_w_3dpw_config.yaml'\n",
        "  ckpt = 'data/pare_w_3dpw_checkpoint.ckpt'\n",
        "  yolo_img_size = 416\n",
        "\n",
        "actual_thing = PARETester(PAREStarter())"
      ],
      "metadata": {
        "id": "FZK_1ligroeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03445a4b-c2a2-46cf-d95b-482ae551016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> init weights from normal distribution\n",
            "IMPORTANT WARNING!! Please download pre-trained models if you are in TRAINING mode!\n",
            "\"Keypoint Attention\" should be activated to be able to use part segmentation\n",
            "Overriding use_keypoint_attention\n",
            "Keypoint attention is active\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Loading pretrained model from data/pare_w_3dpw_checkpoint.ckpt\n",
            "Removing \"model.\" keyword from state_dict keys..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### getbboxes from yolov\n",
        "dets = get_detections(image_file_path)"
      ],
      "metadata": {
        "id": "QzHdfBFQrrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pare_output = actual_thing.run_on_image(resizes_file_path, dets)"
      ],
      "metadata": {
        "id": "qhzfl4O6r2lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PARE-SMPL to STAR"
      ],
      "metadata": {
        "id": "uNrnG4PCr_3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of our Goals was it to improove AvatarGen. One simple improvement was to use the newer and better version of SMPL called STAR.\n",
        "Since PARE unfortunately is fully trained and configured on SMPL we need to convert the created values to create the STAR Model.\n",
        "This implementation is taken directly from the STAR repo and can be found here: https://github.com/ahmedosman/STAR/blob/master/convertors/convert_smpl_to_star.py"
      ],
      "metadata": {
        "id": "gVmsN8Iha74T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "l_QvYqn2sEfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create all the needed helper classes and functions to actually run PARE on the notebook"
      ],
      "metadata": {
        "id": "NLJl5psWbTSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../..')"
      ],
      "metadata": {
        "id": "LmX87dY9j7aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quat2mat(quat):\n",
        "    '''\n",
        "        Converts a quaternion to a rotation matrix\n",
        "    :param quat:\n",
        "    :return:\n",
        "    '''\n",
        "    norm_quat = quat\n",
        "    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n",
        "    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:, 2], norm_quat[:, 3]\n",
        "    B = quat.size(0)\n",
        "    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n",
        "    wx, wy, wz = w * x, w * y, w * z\n",
        "    xy, xz, yz = x * y, x * z, y * z\n",
        "    rotMat = torch.stack([w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz,\n",
        "                          2 * wz + 2 * xy, w2 - x2 + y2 - z2, 2 * yz - 2 * wx,\n",
        "                          2 * xz - 2 * wy, 2 * wx + 2 * yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\n",
        "    return rotMat\n",
        "\n",
        "\n",
        "def quat_feat(theta):\n",
        "    '''\n",
        "        Computes a normalized quaternion ([0,0,0,0]  when the body is in rest pose)\n",
        "        given joint angles\n",
        "    :param theta: A tensor of joints axis angles, batch size x number of joints x 3\n",
        "    :return:\n",
        "    '''\n",
        "    l1norm = torch.norm(theta + 1e-8, p=2, dim=1)\n",
        "    angle = torch.unsqueeze(l1norm, -1)\n",
        "    normalized = torch.div(theta, angle)\n",
        "    angle = angle * 0.5\n",
        "    v_cos = torch.cos(angle)\n",
        "    v_sin = torch.sin(angle)\n",
        "    quat = torch.cat([v_sin * normalized,v_cos-1], dim=1)\n",
        "    return quat\n",
        "\n",
        "def rodrigues(theta):\n",
        "    '''\n",
        "        Computes the rodrigues representation given joint angles\n",
        "\n",
        "    :param theta: batch_size x number of joints x 3\n",
        "    :return: batch_size x number of joints x 3 x 4\n",
        "    '''\n",
        "    l1norm = torch.norm(theta + 1e-8, p = 2, dim = 1)\n",
        "    angle = torch.unsqueeze(l1norm, -1)\n",
        "    normalized = torch.div(theta, angle)\n",
        "    angle = angle * 0.5\n",
        "    v_cos = torch.cos(angle)\n",
        "    v_sin = torch.sin(angle)\n",
        "    quat = torch.cat([v_cos, v_sin * normalized], dim = 1)\n",
        "    return quat2mat(quat)\n",
        "\n",
        "def with_zeros(input):\n",
        "    '''\n",
        "      Appends a row of [0,0,0,1] to a batch size x 3 x 4 Tensor\n",
        "\n",
        "    :param input: A tensor of dimensions batch size x 3 x 4\n",
        "    :return: A tensor batch size x 4 x 4 (appended with 0,0,0,1)\n",
        "    '''\n",
        "    batch_size  = input.shape[0]\n",
        "    row_append     = torch.cuda.FloatTensor(([0.0, 0.0, 0.0, 1.0]))\n",
        "    row_append.requires_grad = False\n",
        "    padded_tensor     = torch.cat([input, row_append.view(1, 1, 4).repeat(batch_size, 1, 1)], 1)\n",
        "    return padded_tensor\n"
      ],
      "metadata": {
        "id": "zzgJdsLhsClF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the STAR class.\n",
        "STAR repo is here: https://github.com/ahmedosman/STAR/tree/master"
      ],
      "metadata": {
        "id": "9Z-XyriFboiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "\n",
        "class STAR(nn.Module):\n",
        "    def __init__(self,gender='female',num_betas=10):\n",
        "        super(STAR, self).__init__()\n",
        "\n",
        "        if gender not in ['male','female','neutral']:\n",
        "            raise RuntimeError('Invalid Gender')\n",
        "\n",
        "        if gender == 'male':\n",
        "            path_model = 'STAR/male/model.npz'\n",
        "        elif gender == 'female':\n",
        "            path_model = 'STAR/female/model.npz'\n",
        "        else:\n",
        "            path_model = 'STAR/neutral/model.npz'\n",
        "\n",
        "        if not os.path.exists(path_model):\n",
        "            raise RuntimeError('Path does not exist %s' % (path_model))\n",
        "        import numpy as np\n",
        "\n",
        "        star_model = np.load(path_model,allow_pickle=True)\n",
        "        J_regressor = star_model['J_regressor']\n",
        "        rows,cols = np.where(J_regressor!=0)\n",
        "        vals = J_regressor[rows,cols]\n",
        "        self.num_betas = num_betas\n",
        "\n",
        "        # Model sparse joints regressor, regresses joints location from a mesh\n",
        "        self.register_buffer('J_regressor', torch.cuda.FloatTensor(J_regressor))\n",
        "\n",
        "        # Model skinning weights\n",
        "        self.register_buffer('weights', torch.cuda.FloatTensor(star_model['weights']))\n",
        "        # Model pose corrective blend shapes\n",
        "        self.register_buffer('posedirs', torch.cuda.FloatTensor(star_model['posedirs'].reshape((-1,93))))\n",
        "        # Mean Shape\n",
        "        self.register_buffer('v_template', torch.cuda.FloatTensor(star_model['v_template']))\n",
        "        # Shape corrective blend shapes\n",
        "        self.register_buffer('shapedirs', torch.cuda.FloatTensor(np.array(star_model['shapedirs'][:,:,:num_betas])))\n",
        "        # Mesh traingles\n",
        "        self.register_buffer('faces', torch.from_numpy(star_model['f'].astype(np.int64)))\n",
        "        self.f = star_model['f']\n",
        "        # Kinematic tree of the model\n",
        "        self.register_buffer('kintree_table', torch.from_numpy(star_model['kintree_table'].astype(np.int64)))\n",
        "\n",
        "        id_to_col = {self.kintree_table[1, i].item(): i for i in range(self.kintree_table.shape[1])}\n",
        "        self.register_buffer('parent', torch.LongTensor(\n",
        "            [id_to_col[self.kintree_table[0, it].item()] for it in range(1, self.kintree_table.shape[1])]))\n",
        "\n",
        "        self.verts = None\n",
        "        self.J = None\n",
        "        self.R = None\n",
        "\n",
        "    def forward(self, pose, betas , trans):\n",
        "        '''\n",
        "            STAR forward pass given pose, betas (shape) and trans\n",
        "            return the model vertices and transformed joints\n",
        "        :param pose: pose  parameters - A batch size x 72 tensor (3 numbers for each joint)\n",
        "        :param beta: beta  parameters - A batch size x number of betas\n",
        "        :param beta: trans parameters - A batch size x 3\n",
        "        :return:\n",
        "                 v         : batch size x 6890 x 3\n",
        "                             The STAR model vertices\n",
        "                 v.v_vposed: batch size x 6890 x 3 model\n",
        "                             STAR vertices in T-pose after adding the shape\n",
        "                             blend shapes and pose blend shapes\n",
        "                 v.v_shaped: batch size x 6890 x 3\n",
        "                             STAR vertices in T-pose after adding the shape\n",
        "                             blend shapes and pose blend shapes\n",
        "                 v.J_transformed:batch size x 24 x 3\n",
        "                                Posed model joints.\n",
        "                 v.f: A numpy array of the model face.\n",
        "        '''\n",
        "        device = pose.device\n",
        "        batch_size = pose.shape[0]\n",
        "        v_template = self.v_template[None, :]\n",
        "        shapedirs  = self.shapedirs.view(-1, self.num_betas)[None, :].expand(batch_size, -1, -1)\n",
        "        beta = betas[:, :, None]\n",
        "        v_shaped = torch.matmul(shapedirs, beta).view(-1, 6890, 3) + v_template\n",
        "        J = torch.einsum('bik,ji->bjk', [v_shaped, self.J_regressor])\n",
        "\n",
        "        pose_quat = quat_feat(pose.view(-1, 3)).view(batch_size, -1)\n",
        "        pose_feat = torch.cat((pose_quat[:,4:],beta[:,1]),1)\n",
        "\n",
        "        R = rodrigues(pose.view(-1, 3)).view(batch_size, 24, 3, 3)\n",
        "        R = R.view(batch_size, 24, 3, 3)\n",
        "\n",
        "        posedirs = self.posedirs[None, :].expand(batch_size, -1, -1)\n",
        "        v_posed = v_shaped + torch.matmul(posedirs, pose_feat[:, :, None]).view(-1, 6890, 3)\n",
        "\n",
        "        J_ = J.clone()\n",
        "        J_[:, 1:, :] = J[:, 1:, :] - J[:, self.parent, :]\n",
        "        G_ = torch.cat([R, J_[:, :, :, None]], dim=-1)\n",
        "        pad_row = torch.FloatTensor([0, 0, 0, 1]).to(device).view(1, 1, 1, 4).expand(batch_size, 24, -1, -1)\n",
        "        G_ = torch.cat([G_, pad_row], dim=2)\n",
        "        G = [G_[:, 0].clone()]\n",
        "        for i in range(1, 24):\n",
        "            G.append(torch.matmul(G[self.parent[i - 1]], G_[:, i, :, :]))\n",
        "        G = torch.stack(G, dim=1)\n",
        "\n",
        "        rest = torch.cat([J, torch.zeros(batch_size, 24, 1).to(device)], dim=2).view(batch_size, 24, 4, 1)\n",
        "        zeros = torch.zeros(batch_size, 24, 4, 3).to(device)\n",
        "        rest = torch.cat([zeros, rest], dim=-1)\n",
        "        rest = torch.matmul(G, rest)\n",
        "        G = G - rest\n",
        "        T = torch.matmul(self.weights, G.permute(1, 0, 2, 3).contiguous().view(24, -1)).view(6890, batch_size, 4,4).transpose(0, 1)\n",
        "        rest_shape_h = torch.cat([v_posed, torch.ones_like(v_posed)[:, :, [0]]], dim=-1)\n",
        "        v = torch.matmul(T, rest_shape_h[:, :, :, None])[:, :, :3, 0]\n",
        "        v = v + trans[:,None,:]\n",
        "        v.f = self.f\n",
        "        v.v_posed = v_posed\n",
        "        v.v_shaped = v_shaped\n",
        "\n",
        "        root_transform = with_zeros(torch.cat((R[:,0],J[:,0][:,:,None]),2))\n",
        "        results =  [root_transform]\n",
        "        for i in range(0, self.parent.shape[0]):\n",
        "            transform_i = with_zeros(torch.cat((R[:, i + 1], J[:, i + 1][:,:,None] - J[:, self.parent[i]][:,:,None]), 2))\n",
        "            curr_res = torch.matmul(results[self.parent[i]],transform_i)\n",
        "            results.append(curr_res)\n",
        "        results = torch.stack(results, dim=1)\n",
        "        posed_joints = results[:, :, :3, 3]\n",
        "        v.J_transformed = posed_joints + trans[:,None,:]\n",
        "        return v\n"
      ],
      "metadata": {
        "id": "n4Wzky7CsUJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### to turn SMPL to STAR\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def get_vert_connectivity(num_verts, mesh_f):\n",
        "    import scipy.sparse as sp\n",
        "    vpv = sp.csc_matrix((num_verts,num_verts))\n",
        "    def row(A):\n",
        "        return A.reshape((1, -1))\n",
        "    def col(A):\n",
        "        return A.reshape((-1, 1))\n",
        "    for i in range(3):\n",
        "        IS = mesh_f[:,i]\n",
        "        JS = mesh_f[:,(i+1)%3]\n",
        "        data = np.ones(len(IS))\n",
        "        ij = np.vstack((row(IS.flatten()), row(JS.flatten())))\n",
        "        mtx = sp.csc_matrix((data, ij), shape=vpv.shape)\n",
        "        vpv = vpv + mtx + mtx.T\n",
        "    return vpv\n",
        "\n",
        "def get_verts_per_edge(num_verts,faces):\n",
        "    import scipy.sparse as sp\n",
        "    vc = sp.coo_matrix(get_vert_connectivity(num_verts, faces))\n",
        "    def row(A):\n",
        "        return A.reshape((1, -1))\n",
        "    def col(A):\n",
        "        return A.reshape((-1, 1))\n",
        "    result = np.hstack((col(vc.row), col(vc.col)))\n",
        "    result = result[result[:,0] < result[:,1]]\n",
        "    return result\n",
        "\n",
        "def edge_loss(d,smpl):\n",
        "    vpe = get_verts_per_edge(6890,d.f)\n",
        "    edges_for = lambda x: x[:,vpe[:,0],:] - x[:,vpe[:,1],:]\n",
        "    edge_obj = edges_for(d) - edges_for(smpl)\n",
        "    return edge_obj\n",
        "\n",
        "def verts_loss(d,smpl):\n",
        "    return torch.sum((d-smpl)**2.0)\n",
        "\n",
        "def convert_smpl_2_star(smpl,MAX_ITER_EDGES,MAX_ITER_VERTS,NUM_BETAS,GENDER):\n",
        "    '''\n",
        "        Convert SMPL meshes to STAR\n",
        "    :param smpl:\n",
        "    :return:\n",
        "    '''\n",
        "    smpl = torch.cuda.FloatTensor(smpl)\n",
        "    batch_size = smpl.shape[0]\n",
        "    if batch_size > 32:\n",
        "        import warnings\n",
        "        warnings.warn(\n",
        "            'The Default optimization parameters (MAX_ITER_EDGES,MAX_ITER_VERTS) were tested on batch size 32 or smaller batches')\n",
        "\n",
        "    star = STAR(gender=GENDER)\n",
        "    global_pose = torch.cuda.FloatTensor(np.zeros((batch_size, 3)))\n",
        "    global_pose = Variable(global_pose, requires_grad=True)\n",
        "    joints_pose = torch.cuda.FloatTensor(np.zeros((batch_size, 72 - 3)))\n",
        "    joints_pose = Variable(joints_pose, requires_grad=True)\n",
        "    betas = torch.cuda.FloatTensor(np.zeros((batch_size, NUM_BETAS)))\n",
        "    betas = Variable(betas, requires_grad=True)\n",
        "    trans = torch.cuda.FloatTensor(np.zeros((batch_size, 3)))\n",
        "    trans = Variable(trans, requires_grad=True)\n",
        "    learning_rate = 1e-1\n",
        "    optimizer = torch.optim.LBFGS([global_pose], lr=learning_rate)\n",
        "    poses = torch.cat((global_pose, joints_pose), 1)\n",
        "    d = star(poses, betas, trans)\n",
        "    ########################################################################################################################\n",
        "    # Fitting the model with an on edges objective first\n",
        "    print('STAGE 1/2 - Fitting the Model on Edges Objective')\n",
        "    for t in range(MAX_ITER_EDGES):\n",
        "        poses = torch.cat((global_pose, joints_pose), 1)\n",
        "        d = star(poses, betas, trans)\n",
        "\n",
        "        def edge_loss_closure():\n",
        "            loss = torch.sum(edge_loss(d, smpl) ** 2.0)\n",
        "            return loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        edge_loss_closure().backward()\n",
        "        optimizer.step(edge_loss_closure)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([joints_pose], lr=learning_rate)\n",
        "    for t in range(MAX_ITER_EDGES):\n",
        "        poses = torch.cat((global_pose, joints_pose), 1)\n",
        "        d = star(poses, betas, trans)\n",
        "\n",
        "        def edge_loss_closure():\n",
        "            loss = torch.sum(edge_loss(d, smpl) ** 2.0)\n",
        "            return loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        edge_loss_closure().backward()\n",
        "        optimizer.step(edge_loss_closure)\n",
        "    ########################################################################################################################\n",
        "    # Fitting the model with an on vertices objective\n",
        "    print('STAGE 2/2 - Fitting the Model on a Vertex Objective')\n",
        "    optimizer = torch.optim.LBFGS([joints_pose, global_pose, trans, betas], lr=learning_rate)\n",
        "    for t in range(MAX_ITER_VERTS):\n",
        "        poses = torch.cat((global_pose, joints_pose), 1)\n",
        "        d = star(poses, betas, trans)\n",
        "\n",
        "        def vertex_closure():\n",
        "            loss = torch.sum(verts_loss(d, smpl) ** 2.0)\n",
        "            return loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        vertex_closure().backward()\n",
        "        optimizer.step(vertex_closure)\n",
        "\n",
        "    ########################################################################################################################\n",
        "    np_poses = poses.detach().cpu().numpy()\n",
        "    np_betas = betas.detach().cpu().numpy()\n",
        "    np_trans = trans.detach().cpu().numpy()\n",
        "    np_star_verts = d.detach().cpu().numpy()\n",
        "    ########################################################################################################################\n",
        "\n",
        "    return np_poses, np_betas, np_trans , np_star_verts"
      ],
      "metadata": {
        "id": "tAuPnBE9scc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Helper functions to get the skinning weights and faces from STAR"
      ],
      "metadata": {
        "id": "CG_63S07b2CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skinning_weitghs(star_gender='female'):\n",
        "  model = STAR(gender=star_gender, num_betas=10)\n",
        "  return model.weights.cpu().numpy()\n",
        "\n",
        "def get_faces(star_gender='female'):\n",
        "  model = STAR(gender=star_gender, num_betas=10)\n",
        "  return model.f"
      ],
      "metadata": {
        "id": "PgCSN78nszFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### actual usage"
      ],
      "metadata": {
        "id": "e0_jUNR8sdx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/AvatarGen_Part1')"
      ],
      "metadata": {
        "id": "nxdOa1bd2uve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### standard parameters for conversion to STAR\n",
        "\n",
        "opt_parms = {'MAX_ITER_EDGES':100 ,\n",
        "             'MAX_ITER_VERTS':1500,\n",
        "             'NUM_BETAS':10,\n",
        "             'GENDER': person_gender}"
      ],
      "metadata": {
        "id": "Yc-xa6ZksfeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smpl_vertices = pare_output.get('smpl_vertices')\n",
        "np_poses, np_betas, np_trans, star_verts = convert_smpl_2_star(smpl_vertices,**opt_parms)"
      ],
      "metadata": {
        "id": "LAzmGFtUshMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938552a9-1717-47fd-dd9c-8abb8dab50bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STAGE 1/2 - Fitting the Model on Edges Objective\n",
            "STAGE 2/2 - Fitting the Model on a Vertex Objective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = STAR(gender=person_gender, num_betas=10)"
      ],
      "metadata": {
        "id": "MhOxF02LspGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.forward(torch.cuda.FloatTensor(np_poses),\n",
        "                      torch.cuda.FloatTensor(np_betas),\n",
        "                      torch.cuda.FloatTensor(np_trans))"
      ],
      "metadata": {
        "id": "evDJ1RTxs2N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the following Data to create our custom converter class"
      ],
      "metadata": {
        "id": "2coGudTXcJMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {'vertices': model.squeeze(0).cpu().numpy(),\n",
        "             'faces': model.f,\n",
        "             'weights': get_skinning_weitghs(),\n",
        "             'pose': np_poses,\n",
        "             'j_transformed': model.J_transformed.cpu().numpy().squeeze(0),\n",
        "             'model_betas': np_betas,\n",
        "             'model_trans': np_trans,\n",
        "             'person_gender': person_gender}"
      ],
      "metadata": {
        "id": "eQpYZU1ptFHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformation to canonical space\n"
      ],
      "metadata": {
        "id": "r2-psT1ntQax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a custom class created by us, which takes the Star Model, and creates and transforms the points needed for StyleSDFs volume rendering.\n",
        "Unfortunately this still runs on CPU, which makes the transformation take a really long time."
      ],
      "metadata": {
        "id": "1fNz5F11cQWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reshaping of model and obtaining d zero values"
      ],
      "metadata": {
        "id": "lHon-ssPepf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CanonicalSpaceConverter:\n",
        "    def __init__(self, vertices: np.ndarray = np.empty([6890, 3]),\n",
        "                 faces: np.ndarray = np.empty([13776, 3]),\n",
        "                 weights: np.ndarray = np.empty([6890, 24]),\n",
        "                 pose: np.ndarray = np.empty([1, 72]),\n",
        "                 j_transformed: np.ndarray = np.empty([24, 3]),\n",
        "                 model_betas = np.empty([1,10]),\n",
        "                 model_trans = np.empty([1,3]),\n",
        "                 person_gender = 'neutral',\n",
        "                 number_of_points: int = 100,\n",
        "                 field_of_sight: int = 6,\n",
        "                 pickle_file: str = ''):\n",
        "\n",
        "        if pickle_file != '':\n",
        "            with open(pickle_file, 'rb') as file:\n",
        "                data = pickle.load(file)\n",
        "            self.vertices = data.get('vertices', vertices)\n",
        "            self.faces = data.get('faces', faces)\n",
        "            self.weights = data.get('weights', weights)\n",
        "            self.pose = data.get('pose', pose)\n",
        "            self.joints = data.get('j_transformed', j_transformed),\n",
        "            self.model_betas = data.get('model_betas', model_betas),\n",
        "            self.model_trans = data.get('model_trans', model_trans),\n",
        "            self.person_gender = data.get('person_gender', person_gender)\n",
        "        else:\n",
        "            self.vertices = vertices\n",
        "            self.faces = faces\n",
        "            self.weights = weights\n",
        "            self.pose = pose\n",
        "            self.joints = j_transformed\n",
        "            self.model_betas = model_betas,\n",
        "            self.model_trans = model_trans,\n",
        "            self.person_gender = person_gender\n",
        "\n",
        "        self.original_verts = self.vertices\n",
        "        self.start_point = self.get_starting_point(0.3, 0.15)\n",
        "        self.points = self.generate_points_cone(number_of_points, 2 * field_of_sight)\n",
        "        self.trimesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\n",
        "        self.neutral_model_params = self.get_neutral_STAR_params()\n",
        "\n",
        "    def get_starting_point(self, azi, pol):\n",
        "        azi = math.radians(280)\n",
        "        pol = math.radians(120)\n",
        "\n",
        "        x = math.sin(azi) * math.cos(pol)\n",
        "        y = math.sin(azi) * math.sin(pol)\n",
        "        z = math.cos(azi)\n",
        "\n",
        "        return np.array([x, y, z])\n",
        "\n",
        "    def create_grid(self, circle_coordinates, cone_radius):\n",
        "      max_values = np.argmax(circle_coordinates, axis=0)\n",
        "      min_values = np.argmin(circle_coordinates, axis=0)\n",
        "\n",
        "      starting_point = np.array([circle_coordinates[min_values[0]][0],\n",
        "                                 circle_coordinates[max_values[1]][1],\n",
        "                                 circle_coordinates[max_values[2]][2]])\n",
        "\n",
        "      end_point = np.array([circle_coordinates[min_values[0]][0],\n",
        "                            circle_coordinates[max_values[1]][1],\n",
        "                            circle_coordinates[min_values[2]][2]])\n",
        "\n",
        "      points_side = np.linspace(starting_point, end_point, 63)\n",
        "\n",
        "      grid = np.empty((64,3))\n",
        "\n",
        "      for start_point in points_side:\n",
        "        grid = np.vstack((grid, np.linspace(start_point, start_point + cone_radius, 64)))\n",
        "\n",
        "      return grid\n",
        "\n",
        "\n",
        "    def generate_points_cone(self, num_points, field_of_view, distance=1, points_on_ray=24):\n",
        "        field_of_view_rad = math.radians(field_of_view)\n",
        "\n",
        "        direction_vector = np.array([0, 0, 0]) - self.start_point\n",
        "        direction_vector /= np.linalg.norm(direction_vector)\n",
        "        self.direction_vector = direction_vector\n",
        "\n",
        "        cone_center = direction_vector * distance\n",
        "        cone_radius = distance * math.tan(field_of_view_rad / 2)\n",
        "\n",
        "        thetas = np.linspace(0, 2*np.pi, num_points, endpoint=False)\n",
        "\n",
        "        end_points = np.empty((0,3),dtype=float)\n",
        "\n",
        "        x = cone_center[0] + cone_radius * np.cos(thetas)\n",
        "        y = cone_center[1] * np.ones_like(x)\n",
        "        z = cone_center[2] + cone_radius * np.sin(thetas)\n",
        "\n",
        "        points = np.column_stack((x, y, z))\n",
        "\n",
        "        end_points = self.create_grid(points, cone_radius)\n",
        "\n",
        "        for point in end_points:\n",
        "            line_points = np.linspace(self.start_point, point, points_on_ray-1)\n",
        "            end_points = np.vstack((end_points, line_points))\n",
        "\n",
        "        self.point_array = end_points\n",
        "\n",
        "        return np.reshape(end_points, (64, 64, 24, 3))\n",
        "\n",
        "    def get_neutral_STAR_params(self):\n",
        "      pose_neutral = torch.cuda.FloatTensor(np.zeros((1,72)))\n",
        "      neutral_model = STAR(gender=self.person_gender, num_betas=10)\n",
        "      neutral_model = neutral_model.forward(pose_neutral,\n",
        "                                            torch.cuda.FloatTensor(self.model_betas[0]),\n",
        "                                            torch.cuda.FloatTensor(self.model_trans))\n",
        "      joints = neutral_model.J_transformed.cpu().numpy().squeeze(0)\n",
        "      verts = neutral_model.squeeze(0).cpu().numpy()\n",
        "      return {'joints': joints[0], 'verts': verts}\n",
        "\n",
        "\n",
        "    def get_direction_nearest_vertice(self, point):\n",
        "        distances = np.linalg.norm(self.vertices - point, axis=1)\n",
        "        closest_vert_ind = np.argmin(distances)\n",
        "\n",
        "        direction_vector = self.vertices[closest_vert_ind] - point\n",
        "        direction_vector /= np.linalg.norm(direction_vector)\n",
        "\n",
        "        return direction_vector\n",
        "\n",
        "    def get_distance_two_points(self, p1, p2):\n",
        "      np1 = np.array(p1)\n",
        "      np2 = np.array(p2)\n",
        "\n",
        "      return np.sqrt(np.sum((np1-np2)**2))\n",
        "\n",
        "    def get_d_zeros(self):\n",
        "      d0 = []\n",
        "\n",
        "      for point in self.point_array:\n",
        "        direction_vector = self.get_direction_nearest_vertice(point)\n",
        "        is_inside = self.trimesh.contains([point])\n",
        "        intersection_point, _, _ = self.trimesh.ray.intersects_location(\n",
        "            ray_origins=[point],\n",
        "            ray_directions=[direction_vector])\n",
        "        if len(intersection_point) > 1: intersection_point = intersection_point[0]\n",
        "        signed_distance = self.get_distance_two_points(point, intersection_point)\n",
        "        if is_inside:\n",
        "          d0.append(-signed_distance)\n",
        "        else:\n",
        "          d0.append(signed_distance)\n",
        "\n",
        "      self.d0 = np.reshape(np.array(d0), (64, 64, 24, 1))\n",
        "\n",
        "      return self.d0\n",
        "\n",
        "    def get_euler_angles(self, point_neutral, point_transformed):\n",
        "      direction_vector =  - point_neutral - point_transformed\n",
        "      direction_vector /= np.linalg.norm(direction_vector)\n",
        "\n",
        "      yaw = np.arctan2(direction_vector[1],direction_vector[0])\n",
        "      pitch = np.arctan2(\n",
        "          np.sqrt(direction_vector[0]**2 + direction_vector[1]**2),\n",
        "          direction_vector[2])\n",
        "      roll = np.arctan2(\n",
        "          np.cos(pitch)*direction_vector[1]+np.sin(pitch) * direction_vector[2],\n",
        "          direction_vector[0])\n",
        "      return np.array([yaw, pitch, roll])\n",
        "\n",
        "\n",
        "    def get_joint_transformation_matrix(self):\n",
        "      translation = []\n",
        "      rotation = []\n",
        "      neutral_joints = self.neutral_model_params.get('joints')\n",
        "      for ind, joint in enumerate(self.joints):\n",
        "        trans = neutral_joints[ind] - joint\n",
        "        eulers = self.get_euler_angles(neutral_joints[ind], joint)\n",
        "        Rx = np.array(([0,0,1],\n",
        "                      [0, np.cos(eulers[0]), -np.sin(eulers[0])],\n",
        "                      [0, np.sin(eulers[0]), np.cos(eulers[0])]))\n",
        "        Ry = np.array(([np.cos(eulers[1]), 0, np.sin(eulers[1])],\n",
        "                      [0,1,0],\n",
        "                      [-np.sin(eulers[1]), 0, np.cos(eulers[1])]))\n",
        "        Rz = np.array(([np.cos(eulers[2]), -np.sin(eulers[2]), 0],\n",
        "                       [np.sin(eulers[2]), np.cos(eulers[2]), 0],\n",
        "                       [0,0,1]))\n",
        "        rotation_matrix = np.dot(np.dot(Rz, Ry), Rx)\n",
        "        translation_vector = np.array(trans)\n",
        "\n",
        "        translation.append(translation_vector)\n",
        "        rotation.append(rotation_matrix)\n",
        "\n",
        "      return np.array(translation), np.array(rotation)\n",
        "\n",
        "    def get_weight_nearest_point(self, point):\n",
        "        distances = np.linalg.norm(self.vertices - point, axis=1)\n",
        "        vert_index = np.argmin(distances)\n",
        "        return self.weights[vert_index]\n",
        "\n",
        "    def transform_point(self, point, translation, rotation):\n",
        "        weights = self.get_weight_nearest_point(point)\n",
        "\n",
        "        transformation = []\n",
        "\n",
        "        for i, weight in enumerate(weights):\n",
        "\n",
        "          transformed_point = weight * (np.dot(rotation[i],  point) + translation[i])\n",
        "          transformation.append(transformed_point)\n",
        "\n",
        "        return np.sum(np.array(transformation), axis=0)\n",
        "\n",
        "    def transform_model(self, full_model=False):\n",
        "      translation, rotation = self.get_joint_transformation_matrix()\n",
        "\n",
        "      transformed_points = []\n",
        "      for point in self.point_array:\n",
        "        transformed_points.append(self.transform_point(point, translation, rotation))\n",
        "\n",
        "      self.point_array = np.array(transformed_points)\n",
        "\n",
        "      if full_model:\n",
        "          self.vertices = self.neutral_model_params.get('verts')[0]\n",
        "          self.trimesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\n",
        "\n",
        "      self.points = np.reshape(self.point_array, (64, 64, 24, 3))\n",
        "\n",
        "      return self.points\n",
        "\n",
        "    def reset_model(self):\n",
        "        self.points = self.generate_points_cone(100, 2 * 12)\n",
        "        self.vertices = self.original_verts\n",
        "        self.trimesh = trimesh.Trimesh(vertices=self.vertices, faces=self.faces)\n",
        "\n",
        "    def show_render(self):\n",
        "        import pyrender\n",
        "        import trimesh\n",
        "\n",
        "        model_mesh = pyrender.Mesh.from_trimesh(self.trimesh)\n",
        "        scene = pyrender.Scene()\n",
        "\n",
        "        line = pyrender.Mesh.from_points(self.points, colors=[1, 0, 0])\n",
        "        scene.add(line)\n",
        "\n",
        "        scene.add(model_mesh)\n",
        "\n",
        "        pyrender.Viewer(scene, use_raymond_lighting=True)\n"
      ],
      "metadata": {
        "id": "bCFOfCVftT5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = CanonicalSpaceConverter(**data_dict)"
      ],
      "metadata": {
        "id": "ZsqK1vJ4trq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_zero = converter.get_d_zeros()"
      ],
      "metadata": {
        "id": "ky9X-E6FxE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = converter.transform_model(full_model=True)"
      ],
      "metadata": {
        "id": "kJNCKsG42HnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get point samples from TriPlane"
      ],
      "metadata": {
        "id": "kEoOn242rNDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the earlier created TriPlane we use the transformed points and sample feature information, which is then later turned into colour and depth features"
      ],
      "metadata": {
        "id": "fnIjR1Lbeil3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_planes():\n",
        "    axis = torch.tensor([[[1, 0, 0],\n",
        "                            [0, 1, 0],\n",
        "                            [0, 0, 1]],\n",
        "                            [[1, 0, 0],\n",
        "                            [0, 0, 1],\n",
        "                            [0, 1, 0]],\n",
        "                            [[0, 0, 1],\n",
        "                            [1, 0, 0],\n",
        "                            [0, 1, 0]]], dtype=torch.float32)\n",
        "    return axis.to(torch.device('cuda', 0))\n",
        "\n",
        "def project_onto_planes(planes, coordinates):\n",
        "    \"\"\"\n",
        "    Does a projection of a 3D point onto a batch of 2D planes,\n",
        "    returning 2D plane coordinates.\n",
        "\n",
        "    Takes plane axes of shape n_planes, 3, 3\n",
        "    # Takes coordinates of shape N, M, 3\n",
        "    # returns projections of shape N*n_planes, M, 2\n",
        "    \"\"\"\n",
        "    N, M, C = coordinates.shape\n",
        "    n_planes, _, _ = planes.shape\n",
        "    coordinates = coordinates.unsqueeze(1).expand(-1, n_planes, -1, -1).reshape(N*n_planes, M, 3)\n",
        "    inv_planes = torch.linalg.inv(planes).unsqueeze(0).expand(N, -1, -1, -1).reshape(N*n_planes, 3, 3)\n",
        "    projections = torch.bmm(coordinates, inv_planes)\n",
        "    return projections[..., :2]\n",
        "\n",
        "def sample_from_planes(plane_features, coordinates):\n",
        "    plane_axes = generate_planes()\n",
        "    N, n_planes, C, H, W = plane_features.shape\n",
        "    _, M, _ = coordinates.shape\n",
        "    plane_features = plane_features.view(N*n_planes, C, H, W)\n",
        "\n",
        "    projected_coordinates = project_onto_planes(plane_axes, coordinates).unsqueeze(1)\n",
        "    output_features = torch.nn.functional.grid_sample(plane_features,\n",
        "                                                      projected_coordinates.float(),\n",
        "                                                      mode='bilinear',\n",
        "                                                      padding_mode='zeros',\n",
        "                                                      align_corners=False)\n",
        "    return output_features.permute(0, 3, 2, 1).reshape(N, n_planes, M, C)\n"
      ],
      "metadata": {
        "id": "MuedqMStrZ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda', 0)\n",
        "coordinates = converter.point_array\n",
        "points_tensor = torch.from_numpy(coordinates).unsqueeze(0).float()\n",
        "points_tensor = points_tensor.to(device)\n",
        "\n",
        "sampled_features = sample_from_planes(triplane[0].unsqueeze(0), points_tensor)"
      ],
      "metadata": {
        "id": "SBjc0jMCrWhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smapled_reshape = sampled_features.reshape(1,3,64,64,24,32)"
      ],
      "metadata": {
        "id": "a3Vrjj9MZIHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create batches for training"
      ],
      "metadata": {
        "id": "RntfFDEQc5SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../AvatarGen_Part1')"
      ],
      "metadata": {
        "id": "uSybHsnunO0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda', 0)"
      ],
      "metadata": {
        "id": "6Jim-iPBz3fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### helper functions for data creation\n",
        "\n",
        "def get_tri_plane(latent_code):\n",
        "\n",
        "  camera = np.array([1., 0., 0., -0.0132909, 0., 1., 0., -0.10394844, 0., 0., 1., -10.351759, 0., 0., 0., 1., 9.784736, 0., 0.5, 0., 4.8875856, 0.5, 0., 0., 1., -3.0198941, 0.01279547, 0.40898454])\n",
        "  camera[16] /= 2\n",
        "  c = torch.zeros(1, 107, device=device)\n",
        "  c[:, :28] = torch.from_numpy(camera).to(device)\n",
        "\n",
        "  Generator = gnarf_pretrained.get('G_ema').cuda()\n",
        "\n",
        "  Generator = copy.deepcopy(Generator).eval().requires_grad_(False).to(device)\n",
        "\n",
        "  ws = Generator.mapping(z=latent_code, c=c)\n",
        "\n",
        "  planes = plane_synthesis(ws=ws, c=c)\n",
        "\n",
        "  return ws, planes.get('planes')\n",
        "\n",
        "def get_converter_params(pose, betas, trans):\n",
        "\n",
        "  star_model = STAR(gender='neutral', num_betas=10)\n",
        "\n",
        "  pose = np.expand_dims(pose, axis=0)\n",
        "  betas = np.expand_dims(betas, axis=0)\n",
        "  trans = np.expand_dims(trans, axis=0)\n",
        "\n",
        "  model = star_model.forward(torch.cuda.FloatTensor(pose),\n",
        "                             torch.cuda.FloatTensor(betas),\n",
        "                             torch.cuda.FloatTensor(trans))\n",
        "\n",
        "  verts = model.squeeze(0).cpu().numpy()\n",
        "  faces = star_model.f\n",
        "  weitghs = star_model.weights.cpu().numpy()\n",
        "  j_transformed = model.J_transformed.cpu().numpy().squeeze(0)\n",
        "\n",
        "\n",
        "  data_dict = {'vertices': verts, 'faces': faces, 'weights': weitghs,\n",
        "             'pose': pose, 'j_transformed': j_transformed, 'model_betas': betas,\n",
        "             'model_trans': trans, 'person_gender': 'neutral'}\n",
        "\n",
        "  return data_dict\n",
        "\n",
        "def get_tri_plane_features(points, triplane):\n",
        "  points_tensor = torch.from_numpy(points).unsqueeze(0).float()\n",
        "  points_tensor = points_tensor.to(device)\n",
        "\n",
        "  sampled_features = sample_from_planes(triplane, points_tensor)\n",
        "\n",
        "  sampled_features = sampled_features.mean(1).reshape(64,64,24,32)\n",
        "\n",
        "  return sampled_features.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "iOZEX-58c-F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(batch_size, seed=1):\n",
        "\n",
        "  # just any seed actually\n",
        "\n",
        "  features = []\n",
        "  d_zeros = []\n",
        "  converted_points = []\n",
        "  gnarf_ws = []\n",
        "\n",
        "  for batch in range(batch_size):\n",
        "    z_rnd = torch.from_numpy(np.random.RandomState(seed).randn(1, 512)).to(device)\n",
        "    pose_rnd = (np.random.rand(72) - 0.5) * 0.4\n",
        "    shape_rnd = (np.random.rand(10) - 0.5) * 0.06\n",
        "    trans = np.zeros(3)\n",
        "\n",
        "    ws, triplane = get_tri_plane(z_rnd)\n",
        "    gnarf_ws.append(ws.cpu().squeeze())\n",
        "\n",
        "    converter_params = get_converter_params(pose_rnd, shape_rnd, trans)\n",
        "    converter = CanonicalSpaceConverter(**converter_params)\n",
        "\n",
        "    d_0 = converter.get_d_zeros()\n",
        "    d_zeros.append(d_0)\n",
        "\n",
        "    points = converter.transform_model(full_model=True)\n",
        "    converted_points.append(points)\n",
        "\n",
        "    coordinates = converter.point_array\n",
        "\n",
        "    tri_plane_features = get_tri_plane_features(coordinates, triplane)\n",
        "\n",
        "    features.append(tri_plane_features)\n",
        "\n",
        "    print(batch)\n",
        "\n",
        "  return {'points': np.stack(converted_points, axis=0),\n",
        "          'd_zeros': np.stack(d_zeros, axis=0),\n",
        "          'features': np.stack(features, axis=0),\n",
        "          'ws': np.stack(gnarf_ws, axis=0)}\n"
      ],
      "metadata": {
        "id": "EVmDaPAJCGr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = get_params(3)"
      ],
      "metadata": {
        "id": "-4UpSjwdmM5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b858a38e-6684-4d8a-d3f5-cf58dd932592",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "0\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "1\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "cuda:0\n",
            "torch.float32\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in params.items():\n",
        "  print(f'key: {k}, type: {type(v)}, shape: {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "QqLUuvFIDTPM",
        "outputId": "856542ca-eeb2-4ec5-f83a-bd046318f70d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7b4148ad766>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'key: {k}, type: {type(v)}, shape: {v.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StyleSDF\n",
        "\n",
        "The training code only updated for the volume rendering training. As we could not add the StyleSDF to the pipeline, the training code for the full-pipeline was not adjusted."
      ],
      "metadata": {
        "id": "eBnl6r-oSWIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### The dataset must be converted into a multi size dataset, convert dataset into lmdb format with sizes 64, 128, 512...\n",
        "###!python prepare_data.py --out_path /content/StyleSDF/data /content/drive/MyDrive/style_sdf_data/images/data --n_worker 4"
      ],
      "metadata": {
        "id": "dRXdbhSZSXsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder 1Drx-TZESQXwuHf6oN480vBV_QMMqDfaN\n",
        "!cp /content/avatargen_stylestf/model.py -d /content/StyleSDF/\n",
        "!cp /content/avatargen_stylestf/volume_renderer.py -d /content/StyleSDF/"
      ],
      "metadata": {
        "id": "7vveGc6Z-hH9",
        "outputId": "0ec2a47d-8421-4260-bf01-756be4ab0b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1-DIwX8bDQ_-4lHeKSgEvlVs39gvkIDg0 model.py\n",
            "Processing file 1-CrMHqXMd3LDWjpKS8reUFAuC8gn23sR volume_renderer.py\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-DIwX8bDQ_-4lHeKSgEvlVs39gvkIDg0\n",
            "To: /content/avatargen_stylestf/model.py\n",
            "100% 33.7k/33.7k [00:00<00:00, 66.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-CrMHqXMd3LDWjpKS8reUFAuC8gn23sR\n",
            "To: /content/avatargen_stylestf/volume_renderer.py\n",
            "100% 15.4k/15.4k [00:00<00:00, 49.3MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd StyleSDF"
      ],
      "metadata": {
        "id": "CrwrOhSvAInO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from losses import *\n",
        "from options import BaseOptions\n",
        "from model import Generator, VolumeRenderDiscriminator\n",
        "from dataset import MultiResolutionDataset\n",
        "from utils import data_sampler, requires_grad, accumulate, sample_data, make_noise, mixing_noise, generate_camera_params\n",
        "from distributed import get_rank, synchronize, reduce_loss_dict, reduce_sum, get_world_size\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "\n",
        "def train(opt, experiment_opt, loader, generator, discriminator, g_optim, d_optim, g_ema, device):\n",
        "    loader = sample_data(loader)\n",
        "\n",
        "    mean_path_length = 0\n",
        "\n",
        "    d_loss_val = 0\n",
        "    r1_loss = torch.tensor(0.0, device=device)\n",
        "    d_view_loss = torch.tensor(0.0, device=device)\n",
        "    g_view_loss = torch.tensor(0.0, device=device)\n",
        "    g_eikonal = torch.tensor(0.0, device=device)\n",
        "    g_minimal_surface = torch.tensor(0.0, device=device)\n",
        "\n",
        "    g_loss_val = 0\n",
        "    loss_dict = {}\n",
        "\n",
        "    viewpoint_condition = opt.view_lambda > 0\n",
        "\n",
        "    if opt.distributed:\n",
        "        g_module = generator.module\n",
        "        d_module = discriminator.module\n",
        "    else:\n",
        "        g_module = generator\n",
        "        d_module = discriminator\n",
        "\n",
        "    accum = 0.5 ** (32 / (10 * 1000))\n",
        "\n",
        "    sample_features = [torch.randn(3,64,64,24,32).to(device).repeat_interleave(8,dim=0)] ### For 8 different angle\n",
        "    sample_pts = [torch.randn(3, 64,64,24,3).to(device).repeat_interleave(8,dim=0)]\n",
        "    sample_d0s = [torch.randn(3, 64,64,24,1).to(device).repeat_interleave(8,dim=0)]\n",
        "    sample_ws = [torch.randn(3, opt.style_dim, 2).to(device).repeat_interleave(8,dim=0)]\n",
        "\n",
        "    sample_cam_extrinsics, sample_focals, sample_near, sample_far, _ = generate_camera_params(opt.renderer_output_size, device, batch=opt.val_n_sample, sweep=True,\n",
        "                                                                                         uniform=opt.camera.uniform, azim_range=opt.camera.azim,\n",
        "                                                                                         elev_range=opt.camera.elev, fov_ang=opt.camera.fov,\n",
        "                                                                                         dist_radius=opt.camera.dist_radius)\n",
        "\n",
        "    if opt.with_sdf and opt.sphere_init and opt.start_iter == 0:\n",
        "        init_pbar = range(10000)\n",
        "        if get_rank() == 0:\n",
        "            init_pbar = tqdm(init_pbar, initial=0, dynamic_ncols=True, smoothing=0.01)\n",
        "\n",
        "        generator.zero_grad()\n",
        "        for idx in init_pbar:\n",
        "            init_features = torch.randn(3,64,64,24,32).to(device)\n",
        "            init_pts = torch.randn(3, 64,64,24,3).to(device)\n",
        "            init_d0s = torch.randn(3, 64,64,24,1).to(device)\n",
        "            init_ws = torch.randn(3, opt.style_dim, 2).to(device)\n",
        "            cam_extrinsics, focal, near, far, gt_viewpoints = generate_camera_params(opt.renderer_output_size, device, batch=3,\n",
        "                                                                                uniform=opt.camera.uniform, azim_range=opt.camera.azim,\n",
        "                                                                                elev_range=opt.camera.elev, fov_ang=opt.camera.fov,\n",
        "                                                                                dist_radius=opt.camera.dist_radius)\n",
        "            sdf, target_values = g_module.init_forward(init_features, init_pts, init_d0s, init_ws, cam_extrinsics, focal, near, far)\n",
        "            loss = F.l1_loss(sdf, target_values)\n",
        "            loss.backward()\n",
        "            g_optim.step()\n",
        "            generator.zero_grad()\n",
        "            if get_rank() == 0:\n",
        "                init_pbar.set_description((f\"MLP init to sphere procedure - Loss: {loss.item():.4f}\"))\n",
        "\n",
        "        accumulate(g_ema, g_module, 0)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"g\": g_module.state_dict(),\n",
        "                \"d\": d_module.state_dict(),\n",
        "                \"g_ema\": g_ema.state_dict(),\n",
        "            },\n",
        "            os.path.join(opt.checkpoints_dir, experiment_opt.expname, f\"sdf_init_models_{str(0).zfill(7)}.pt\")\n",
        "        )\n",
        "        print('Successfully saved checkpoint for SDF initialized MLP.')\n",
        "\n",
        "    pbar = range(opt.iter)\n",
        "    if get_rank() == 0:\n",
        "        pbar = tqdm(pbar, initial=opt.start_iter, dynamic_ncols=True, smoothing=0.01)\n",
        "\n",
        "    for idx in pbar:\n",
        "        i = idx + opt.start_iter\n",
        "\n",
        "        if i > opt.iter:\n",
        "            print(\"Done!\")\n",
        "\n",
        "            break\n",
        "\n",
        "        requires_grad(generator, False)\n",
        "        requires_grad(discriminator, True)\n",
        "        discriminator.zero_grad()\n",
        "        _, real_imgs = next(loader)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        params = get_params(opt.batch)\n",
        "        features = params[\"features\"]\n",
        "        d_zeros = params[\"d_zeros\"]\n",
        "        pts = params[\"points\"]\n",
        "        ws = params[\"ws\"]\n",
        "        batch_features = torch.from_numpy(features)\n",
        "        batch_pts = torch.from_numpy(pts)\n",
        "        batch_d_zeros = torch.from_numpy(d_zeros)\n",
        "        batch_ws = torch.from_numpy(ws)\n",
        "        cam_extrinsics, focal, near, far, gt_viewpoints = generate_camera_params(opt.renderer_output_size, device, batch=opt.batch,\n",
        "                                                                            uniform=opt.camera.uniform, azim_range=opt.camera.azim,\n",
        "                                                                            elev_range=opt.camera.elev, fov_ang=opt.camera.fov,\n",
        "                                                                            dist_radius=opt.camera.dist_radius)\n",
        "        ### Discriminator training\n",
        "        gen_imgs = []\n",
        "        for j in range(0, opt.batch, opt.chunk):\n",
        "            points = batch_pts[j:j+opt.chunk]\n",
        "            tmp_features = batch_features[j:j+opt.chunk]\n",
        "            tmp_sdf = batch_d_zeros[j:j+opt.chunk]\n",
        "            tmp_ws = batch_ws[j:j+opt.chunk]\n",
        "            _, fake_img = generator(tmp_features, points, tmp_ws, tmp_sdf,\n",
        "                                    cam_extrinsics[j:j+opt.chunk],\n",
        "                                    focal[j:j+opt.chunk],\n",
        "                                    near[j:j+opt.chunk],\n",
        "                                    far[j:j+opt.chunk])\n",
        "\n",
        "            gen_imgs += [fake_img]\n",
        "\n",
        "        gen_imgs = torch.cat(gen_imgs, 0)\n",
        "        fake_pred, fake_viewpoint_pred = discriminator(gen_imgs.detach())\n",
        "        if viewpoint_condition:\n",
        "            d_view_loss = opt.view_lambda * viewpoints_loss(fake_viewpoint_pred, gt_viewpoints)\n",
        "\n",
        "        real_imgs.requires_grad = True\n",
        "        real_pred, _ = discriminator(real_imgs)\n",
        "        d_gan_loss = d_logistic_loss(real_pred, fake_pred)\n",
        "        grad_penalty = d_r1_loss(real_pred, real_imgs)\n",
        "        r1_loss = opt.r1 * 0.5 * grad_penalty\n",
        "        d_loss = d_gan_loss + r1_loss + d_view_loss\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "        loss_dict[\"d\"] = d_gan_loss\n",
        "        loss_dict[\"r1\"] = r1_loss\n",
        "        loss_dict[\"d_view\"] = d_view_loss\n",
        "        loss_dict[\"real_score\"] = real_pred.mean()\n",
        "        loss_dict[\"fake_score\"] = fake_pred.mean()\n",
        "\n",
        "        requires_grad(generator, True)\n",
        "        requires_grad(discriminator, False)\n",
        "\n",
        "        ### Generator training\n",
        "        for j in range(0, opt.batch, opt.chunk):\n",
        "            points = batch_pts[j:j+opt.chunk]\n",
        "            tmp_features = batch_features[j:j+opt.chunk]\n",
        "            tmp_sdf = batch_d_zeros[j:j+opt.chunk]\n",
        "            tmp_ws = batch_ws[j:j+opt.chunk]\n",
        "            cam_extrinsics, focal, near, far, curr_gt_viewpoints = generate_camera_params(opt.renderer_output_size, device, batch=opt.chunk,\n",
        "                                                                                     uniform=opt.camera.uniform, azim_range=opt.camera.azim,\n",
        "                                                                                     elev_range=opt.camera.elev, fov_ang=opt.camera.fov,\n",
        "                                                                                     dist_radius=opt.camera.dist_radius)\n",
        "\n",
        "            out = generator(tmp_features, points, tmp_ws, tmp_sdf, cam_extrinsics, focal, near, far,\n",
        "                            return_sdf=opt.min_surf_lambda > 0,\n",
        "                            return_eikonal=opt.eikonal_lambda > 0)\n",
        "            fake_img  = out[1]\n",
        "            if opt.min_surf_lambda > 0:\n",
        "                sdf = out[2]\n",
        "            if opt.eikonal_lambda > 0:\n",
        "                eikonal_term = out[3]\n",
        "\n",
        "            fake_pred, fake_viewpoint_pred = discriminator(fake_img)\n",
        "            if viewpoint_condition:\n",
        "                g_view_loss = opt.view_lambda * viewpoints_loss(fake_viewpoint_pred, curr_gt_viewpoints)\n",
        "\n",
        "            if opt.with_sdf and opt.eikonal_lambda > 0:\n",
        "                g_eikonal, g_minimal_surface = eikonal_loss(eikonal_term, sdf=sdf if opt.min_surf_lambda > 0 else None,\n",
        "                                                            beta=opt.min_surf_beta)\n",
        "                g_eikonal = opt.eikonal_lambda * g_eikonal\n",
        "                if opt.min_surf_lambda > 0:\n",
        "                    g_minimal_surface = opt.min_surf_lambda * g_minimal_surface\n",
        "\n",
        "            g_gan_loss = g_nonsaturating_loss(fake_pred)\n",
        "            g_loss = g_gan_loss + g_view_loss + g_eikonal + g_minimal_surface\n",
        "            g_loss.backward()\n",
        "\n",
        "        g_optim.step()\n",
        "        generator.zero_grad()\n",
        "        loss_dict[\"g\"] = g_gan_loss\n",
        "        loss_dict[\"g_view\"] = g_view_loss\n",
        "        loss_dict[\"g_eikonal\"] = g_eikonal\n",
        "        loss_dict[\"g_minimal_surface\"] = g_minimal_surface\n",
        "\n",
        "        accumulate(g_ema, g_module, accum)\n",
        "\n",
        "        loss_reduced = reduce_loss_dict(loss_dict)\n",
        "        d_loss_val = loss_reduced[\"d\"].mean().item()\n",
        "        g_loss_val = loss_reduced[\"g\"].mean().item()\n",
        "        r1_val = loss_reduced[\"r1\"].mean().item()\n",
        "        real_score_val = loss_reduced[\"real_score\"].mean().item()\n",
        "        fake_score_val = loss_reduced[\"fake_score\"].mean().item()\n",
        "        d_view_val = loss_reduced[\"d_view\"].mean().item()\n",
        "        g_view_val = loss_reduced[\"g_view\"].mean().item()\n",
        "        g_eikonal_loss = loss_reduced[\"g_eikonal\"].mean().item()\n",
        "        g_minimal_surface_loss = loss_reduced[\"g_minimal_surface\"].mean().item()\n",
        "        g_beta_val = g_module.renderer.sigmoid_beta.item() if opt.with_sdf else 0\n",
        "\n",
        "        if get_rank() == 0:\n",
        "            pbar.set_description(\n",
        "                (f\"d: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; viewpoint: {d_view_val+g_view_val:.4f}; eikonal: {g_eikonal_loss:.4f}; surf: {g_minimal_surface_loss:.4f}\")\n",
        "            )\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                with torch.no_grad():\n",
        "                    samples = torch.Tensor(0, 3, opt.renderer_output_size, opt.renderer_output_size)\n",
        "                    step_size = 4\n",
        "                    mean_latent = g_module.mean_latent(10000, device)\n",
        "                    for k in range(0, opt.val_n_sample * 8, step_size):\n",
        "                        _, curr_samples = g_ema([sample_z[0][k:k+step_size]],\n",
        "                                                sample_cam_extrinsics[k:k+step_size],\n",
        "                                                sample_focals[k:k+step_size],\n",
        "                                                sample_near[k:k+step_size],\n",
        "                                                sample_far[k:k+step_size],\n",
        "                                                truncation=0.7,\n",
        "                                                truncation_latent=mean_latent,)\n",
        "                        samples = torch.cat([samples, curr_samples.cpu()], 0)\n",
        "\n",
        "                    if i % 10000 == 0:\n",
        "                        utils.save_image(samples,\n",
        "                            os.path.join(opt.checkpoints_dir, experiment_opt.expname, 'volume_renderer', f\"samples/{str(i).zfill(7)}.png\"),\n",
        "                            nrow=int(opt.val_n_sample),\n",
        "                            normalize=True,\n",
        "                            value_range=(-1, 1),)\n",
        "\n",
        "            if wandb and opt.wandb:\n",
        "                wandb_log_dict = {\"Generator\": g_loss_val,\n",
        "                                  \"Discriminator\": d_loss_val,\n",
        "                                  \"R1\": r1_val,\n",
        "                                  \"Real Score\": real_score_val,\n",
        "                                  \"Fake Score\": fake_score_val,\n",
        "                                  \"D viewpoint\": d_view_val,\n",
        "                                  \"G viewpoint\": g_view_val,\n",
        "                                  \"G eikonal loss\": g_eikonal_loss,\n",
        "                                  \"G minimal surface loss\": g_minimal_surface_loss,\n",
        "                                  }\n",
        "                if opt.with_sdf:\n",
        "                    wandb_log_dict.update({\"Beta value\": g_beta_val})\n",
        "\n",
        "                if i % 1000 == 0:\n",
        "                    wandb_grid = utils.make_grid(samples, nrow=int(opt.val_n_sample),\n",
        "                                                   normalize=True, value_range=(-1, 1))\n",
        "                    wandb_ndarr = (255 * wandb_grid.permute(1, 2, 0).numpy()).astype(np.uint8)\n",
        "                    wandb_images = Image.fromarray(wandb_ndarr)\n",
        "                    wandb_log_dict.update({\"examples\": [wandb.Image(wandb_images,\n",
        "                                            caption=\"Generated samples for azimuth angles of: -35, -25, -15, -5, 5, 15, 25, 35 degrees.\")]})\n",
        "\n",
        "                wandb.log(wandb_log_dict)\n",
        "\n",
        "            if i % 10000 == 0 or (i < 10000 and i % 1000 == 0):\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"g\": g_module.state_dict(),\n",
        "                        \"d\": d_module.state_dict(),\n",
        "                        \"g_ema\": g_ema.state_dict(),\n",
        "                    },\n",
        "                    os.path.join(opt.checkpoints_dir, experiment_opt.expname, 'volume_renderer', f\"models_{str(i).zfill(7)}.pt\")\n",
        "                )\n",
        "                print('Successfully saved checkpoint for iteration {}.'.format(i))\n",
        "\n",
        "    if get_rank() == 0:\n",
        "        # create final model directory\n",
        "        final_model_path = 'pretrained_renderer'\n",
        "        os.makedirs(final_model_path, exist_ok=True)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"g\": g_module.state_dict(),\n",
        "                \"d\": d_module.state_dict(),\n",
        "                \"g_ema\": g_ema.state_dict(),\n",
        "            },\n",
        "            os.path.join(final_model_path, experiment_opt.expname + '_vol_renderer.pt')\n",
        "        )\n",
        "        print('Successfully saved final model.')"
      ],
      "metadata": {
        "id": "YbBZ4kBgoLKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "opt = BaseOptions().parse()\n",
        "opt.model.freeze_renderer = False\n",
        "opt.model.no_viewpoint_loss = opt.training.view_lambda == 0.0\n",
        "opt.training.camera = opt.camera\n",
        "opt.training.renderer_output_size = opt.model.renderer_spatial_output_dim\n",
        "opt.training.style_dim = opt.model.style_dim\n",
        "opt.training.with_sdf = not opt.rendering.no_sdf\n",
        "if opt.training.with_sdf and opt.training.min_surf_lambda > 0:\n",
        "    opt.rendering.return_sdf = True\n",
        "opt.training.iter = 200001\n",
        "opt.rendering.no_features_output = True\n",
        "\n",
        "### Set options for AvatarGen training\n",
        "opt.batch = 16\n",
        "opt.chunk = 8\n",
        "opt.style_dim = 32 ### Triplane feature dim per plane\n",
        "\n",
        "n_gpu = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "opt.training.distributed = n_gpu > 1\n",
        "\n",
        "if opt.training.distributed:\n",
        "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
        "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "    synchronize()\n",
        "\n",
        "# create checkpoints directories\n",
        "os.makedirs(os.path.join(opt.training.checkpoints_dir, opt.experiment.expname, 'volume_renderer'), exist_ok=True)\n",
        "os.makedirs(os.path.join(opt.training.checkpoints_dir, opt.experiment.expname, 'volume_renderer', 'samples'), exist_ok=True)\n",
        "\n",
        "discriminator = VolumeRenderDiscriminator(opt.model).to(device)\n",
        "generator = Generator(opt.model, opt.rendering, full_pipeline=False).to(device)\n",
        "g_ema = Generator(opt.model, opt.rendering, ema=True, full_pipeline=False).to(device)\n",
        "\n",
        "g_ema.eval()\n",
        "accumulate(g_ema, generator, 0)\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-5, betas=(0, 0.9))\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0, 0.9))\n",
        "\n",
        "opt.training.start_iter = 0\n",
        "\n",
        "if opt.experiment.continue_training and opt.experiment.ckpt is not None:\n",
        "    if get_rank() == 0:\n",
        "        print(\"load model:\", opt.experiment.ckpt)\n",
        "    ckpt_path = os.path.join(opt.training.checkpoints_dir,\n",
        "                              opt.experiment.expname,\n",
        "                              'models_{}.pt'.format(opt.experiment.ckpt.zfill(7)))\n",
        "    ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    try:\n",
        "        opt.training.start_iter = int(opt.experiment.ckpt) + 1\n",
        "\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    generator.load_state_dict(ckpt[\"g\"])\n",
        "    discriminator.load_state_dict(ckpt[\"d\"])\n",
        "    g_ema.load_state_dict(ckpt[\"g_ema\"])\n",
        "    if \"g_optim\" in ckpt.keys():\n",
        "        g_optim.load_state_dict(ckpt[\"g_optim\"])\n",
        "        d_optim.load_state_dict(ckpt[\"d_optim\"])\n",
        "\n",
        "sphere_init_path = './pretrained_renderer/sphere_init.pt'\n",
        "if opt.training.no_sphere_init:\n",
        "    opt.training.sphere_init = False\n",
        "elif not opt.experiment.continue_training and opt.training.with_sdf and os.path.isfile(sphere_init_path):\n",
        "    if get_rank() == 0:\n",
        "        print(\"loading sphere inititialized model\")\n",
        "    ckpt = torch.load(sphere_init_path, map_location=lambda storage, loc: storage)\n",
        "    generator.load_state_dict(ckpt[\"g\"])\n",
        "    discriminator.load_state_dict(ckpt[\"d\"])\n",
        "    g_ema.load_state_dict(ckpt[\"g_ema\"])\n",
        "    opt.training.sphere_init = False\n",
        "else:\n",
        "    opt.training.sphere_init = True\n",
        "\n",
        "if opt.training.distributed:\n",
        "    generator = nn.parallel.DistributedDataParallel(\n",
        "        generator,\n",
        "        device_ids=[opt.training.local_rank],\n",
        "        output_device=opt.training.local_rank,\n",
        "        broadcast_buffers=True,\n",
        "        find_unused_parameters=True,\n",
        "    )\n",
        "\n",
        "    discriminator = nn.parallel.DistributedDataParallel(\n",
        "        discriminator,\n",
        "        device_ids=[opt.training.local_rank],\n",
        "        output_device=opt.training.local_rank,\n",
        "        broadcast_buffers=False,\n",
        "        find_unused_parameters=True\n",
        "    )\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)])\n",
        "\n",
        "dataset = MultiResolutionDataset(opt.dataset.dataset_path, transform, opt.model.size,\n",
        "                                  opt.model.renderer_spatial_output_dim)\n",
        "loader = data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=opt.training.batch,\n",
        "    sampler=data_sampler(dataset, shuffle=True, distributed=opt.training.distributed),\n",
        "    drop_last=True,\n",
        ")\n",
        "opt.training.dataset_name = opt.dataset.dataset_path.lower()\n",
        "\n",
        "# save options\n",
        "opt_path = os.path.join(opt.training.checkpoints_dir, opt.experiment.expname, 'volume_renderer', f\"opt.yaml\")\n",
        "with open(opt_path,'w') as f:\n",
        "    yaml.safe_dump(opt, f)\n",
        "\n",
        "# set wandb environment\n",
        "if get_rank() == 0 and wandb is not None and opt.training.wandb:\n",
        "    wandb.init(project=\"StyleSDF\")\n",
        "    wandb.run.name = opt.experiment.expname\n",
        "    wandb.config.dataset = os.path.basename(opt.dataset.dataset_path)\n",
        "    wandb.config.update(opt.training)\n",
        "    wandb.config.update(opt.model)\n",
        "    wandb.config.update(opt.rendering)\n",
        "\n",
        "train(opt.training, opt.experiment, loader, generator, discriminator, g_optim, d_optim, g_ema, device)"
      ],
      "metadata": {
        "id": "hiEf3Ww0oVvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of the StyleSDF on the Deep Fashion Data"
      ],
      "metadata": {
        "id": "tOXSyTiA90uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code snippet below is used for StyleSDF training on the Deep-Fashion Multi-Modal dataset."
      ],
      "metadata": {
        "id": "FyTDRC_b9ezh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd StyleSDF ### If not already in StyleSDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odJCuJd4E0Xi",
        "outputId": "aa4764b0-d776-44de-c04b-aeea4829e1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/StyleSDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset can be reached with a request via email."
      ],
      "metadata": {
        "id": "s7ePdqTjBQM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_volume_renderer.py --batch 16 --chunk 8 --expname fashion --dataset_path /content/drive/MyDrive/style_sdf_data/data --size 512 --wandb"
      ],
      "metadata": {
        "id": "LQxX-tJDSnIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_full_pipeline.py --batch 16 --chunk 8 --expname fashion --size 512 --wandb --dataset_path /content/drive/MyDrive/style_sdf_data/data"
      ],
      "metadata": {
        "id": "Vgd--RFS9tjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with trained StyleSDF\n",
        "\n",
        "Following inference code is taken from the authors implementation. We could not yet add StyleSDF into the AvatarGen pipeline. That's why, we also did not update the inference code accordingly."
      ],
      "metadata": {
        "id": "UnLyFkWlLv8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "checkpoint_it = 70000\n",
        "\n",
        "if checkpoint_it == 10000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1Pg90JZbR7i9B4c6ws69t9X-tfh8sBYd3\")\n",
        "elif checkpoint_it == 20000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1-0bMHN1RRDZvOSm5IcaCymkd4ulHW8Ke\")\n",
        "elif checkpoint_it == 30000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1-B3tdU1G7NK7SDlsCrZ0tcIFC3sjf79D\")\n",
        "elif checkpoint_it == 40000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=18UEJ4mZ23VHwZmsQez7NmQKKuumfGmAk\")\n",
        "elif checkpoint_it == 50000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1-1rWLZnC11xZrfI1N9jidaAHO0rsdEsf\")\n",
        "elif checkpoint_it == 60000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1-48ZACFEuSSJKkNSYJjBEdE1m7mt7qwy\")\n",
        "elif checkpoint_it == 70000:\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1-6nLJR323Ul2hwuyP7_GQSuohpLZKcOm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNaIUMHlC_xy",
        "outputId": "96e782af-2b45-4680-b338-fba98660abf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-6nLJR323Ul2hwuyP7_GQSuohpLZKcOm\n",
            "To: /content/StyleSDF/models_0070000.pt\n",
            "100%|██████████| 185M/185M [00:01<00:00, 118MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "shutil.copy(\"/content/StyleSDF/models_00\" + str(checkpoint_it) + \".pt\", \"/content/StyleSDF/full_models/\")\n",
        "os.rename(\"/content/StyleSDF/full_models/models_00\" + str(checkpoint_it) + \".pt\", \"/content/StyleSDF/full_models/fashion.pt\")"
      ],
      "metadata": {
        "id": "cfEayGnTDUes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "from munch import *\n",
        "from options import BaseOptions\n",
        "from model import Generator\n",
        "from generate_shapes_and_images import generate\n",
        "from render_video import render_video\n",
        "\n",
        "\n",
        "torch.random.manual_seed(321)\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "opt = BaseOptions().parse()\n",
        "opt.camera.uniform = True\n",
        "opt.model.is_test = True\n",
        "opt.model.freeze_renderer = False\n",
        "opt.rendering.offset_sampling = True\n",
        "opt.rendering.static_viewdirs = True\n",
        "opt.rendering.force_background = True\n",
        "opt.rendering.perturb = 0\n",
        "opt.inference.renderer_output_size = opt.model.renderer_spatial_output_dim\n",
        "opt.inference.style_dim = opt.model.style_dim\n",
        "opt.inference.project_noise = opt.model.project_noise"
      ],
      "metadata": {
        "id": "vikE956XEoth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User options\n",
        "opt.inference.no_surface_renderings = False # When true, only RGB images will be created\n",
        "opt.inference.fixed_camera_angles = False # When true, each identity will be rendered from a specific set of 13 viewpoints. Otherwise, random views are generated\n",
        "opt.inference.identities = 4 # Number of identities to generate\n",
        "opt.inference.num_views_per_id = 1 # Number of viewpoints generated per identity. This option is ignored if opt.inference.fixed_camera_angles is true.\n",
        "\n",
        "model_path = 'fashion.pt'\n",
        "opt.model.size = 512\n",
        "opt.experiment.expname = 'fashion'\n",
        "\n",
        "# Create results directory\n",
        "result_model_dir = 'final_model'\n",
        "results_dir_basename = os.path.join(opt.inference.results_dir, opt.experiment.expname)\n",
        "opt.inference.results_dst_dir = os.path.join(results_dir_basename, result_model_dir)\n",
        "if opt.inference.fixed_camera_angles:\n",
        "    opt.inference.results_dst_dir = os.path.join(opt.inference.results_dst_dir, 'fixed_angles')\n",
        "else:\n",
        "    opt.inference.results_dst_dir = os.path.join(opt.inference.results_dst_dir, 'random_angles')\n",
        "\n",
        "os.makedirs(opt.inference.results_dst_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(opt.inference.results_dst_dir, 'images'), exist_ok=True)\n",
        "if not opt.inference.no_surface_renderings:\n",
        "    os.makedirs(os.path.join(opt.inference.results_dst_dir, 'depth_map_meshes'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(opt.inference.results_dst_dir, 'marching_cubes_meshes'), exist_ok=True)\n",
        "\n",
        "opt.inference.camera = opt.camera\n",
        "opt.inference.size = opt.model.size\n",
        "checkpoint_path = os.path.join('full_models', model_path)\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load image generation model\n",
        "g_ema = Generator(opt.model, opt.rendering).to(device)\n",
        "pretrained_weights_dict = checkpoint[\"g_ema\"]\n",
        "model_dict = g_ema.state_dict()\n",
        "for k, v in pretrained_weights_dict.items():\n",
        "    if v.size() == model_dict[k].size():\n",
        "        model_dict[k] = v\n",
        "\n",
        "g_ema.load_state_dict(model_dict)\n",
        "\n",
        "# Load a second volume renderer that extracts surfaces at 128x128x128 (or higher) for better surface resolution\n",
        "if not opt.inference.no_surface_renderings:\n",
        "    opt['surf_extraction'] = Munch()\n",
        "    opt.surf_extraction.rendering = opt.rendering\n",
        "    opt.surf_extraction.model = opt.model.copy()\n",
        "    opt.surf_extraction.model.renderer_spatial_output_dim = 128\n",
        "    opt.surf_extraction.rendering.N_samples = opt.surf_extraction.model.renderer_spatial_output_dim\n",
        "    opt.surf_extraction.rendering.return_xyz = True\n",
        "    opt.surf_extraction.rendering.return_sdf = True\n",
        "    surface_g_ema = Generator(opt.surf_extraction.model, opt.surf_extraction.rendering, full_pipeline=False).to(device)\n",
        "\n",
        "\n",
        "    # Load weights to surface extractor\n",
        "    surface_extractor_dict = surface_g_ema.state_dict()\n",
        "    for k, v in pretrained_weights_dict.items():\n",
        "        if k in surface_extractor_dict.keys() and v.size() == surface_extractor_dict[k].size():\n",
        "            surface_extractor_dict[k] = v\n",
        "\n",
        "    surface_g_ema.load_state_dict(surface_extractor_dict)\n",
        "else:\n",
        "    surface_g_ema = None\n",
        "\n",
        "# Get the mean latent vector for g_ema\n",
        "if opt.inference.truncation_ratio < 1:\n",
        "    with torch.no_grad():\n",
        "        mean_latent = g_ema.mean_latent(opt.inference.truncation_mean, device)\n",
        "else:\n",
        "    surface_mean_latent = None\n",
        "\n",
        "# Get the mean latent vector for surface_g_ema\n",
        "if not opt.inference.no_surface_renderings:\n",
        "    surface_mean_latent = mean_latent[0]\n",
        "else:\n",
        "    surface_mean_latent = None"
      ],
      "metadata": {
        "id": "u6l9z8Y9EDwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(opt.inference, g_ema, surface_g_ema, device, mean_latent, surface_mean_latent)"
      ],
      "metadata": {
        "id": "QxtY0MP-FBye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from trimesh.viewer.notebook import scene_to_html as mesh2html\n",
        "from IPython.display import HTML as viewer_html\n",
        "\n",
        "# First let's look at the images\n",
        "img_dir = os.path.join(opt.inference.results_dst_dir,'images')\n",
        "im_list = sorted([entry for entry in os.listdir(img_dir) if 'thumb' not in entry])\n",
        "img = Image.new('RGB', (256 * len(im_list), 256))\n",
        "for i, im_file in enumerate(im_list):\n",
        "    im_path = os.path.join(img_dir, im_file)\n",
        "    curr_img = Image.open(im_path).resize((256,256)) # the displayed image is scaled to fit to the screen\n",
        "    img.paste(curr_img, (256 * i, 0))\n",
        "\n",
        "display(img)\n",
        "\n",
        "# And now, we'll move on to display the marching cubes and depth map meshes\n",
        "\n",
        "marching_cubes_meshes_dir = os.path.join(opt.inference.results_dst_dir,'marching_cubes_meshes')\n",
        "marching_cubes_meshes_list = sorted([os.path.join(marching_cubes_meshes_dir, entry) for entry in os.listdir(marching_cubes_meshes_dir) if 'obj' in entry])\n",
        "depth_map_meshes_dir = os.path.join(opt.inference.results_dst_dir,'depth_map_meshes')\n",
        "depth_map_meshes_list = sorted([os.path.join(depth_map_meshes_dir, entry) for entry in os.listdir(depth_map_meshes_dir) if 'obj' in entry])\n",
        "for i, mesh_files in enumerate(zip(marching_cubes_meshes_list, depth_map_meshes_list)):\n",
        "    mc_mesh_file, dm_mesh_file = mesh_files[0], mesh_files[1]\n",
        "    marching_cubes_mesh = trimesh.Scene(trimesh.load_mesh(mc_mesh_file, 'obj'))\n",
        "    curr_mc_html = mesh2html(marching_cubes_mesh).replace('\"', '&quot;')\n",
        "    display(viewer_html(' '.join(['<iframe srcdoc=\"{srcdoc}\"',\n",
        "                            'width=\"{width}px\" height=\"{height}px\"',\n",
        "                            'style=\"border:none;\"></iframe>']).format(\n",
        "                            srcdoc=curr_mc_html, height=256, width=256)))\n",
        "    depth_map_mesh = trimesh.Scene(trimesh.load_mesh(dm_mesh_file, 'obj'))\n",
        "    curr_dm_html = mesh2html(depth_map_mesh).replace('\"', '&quot;')\n",
        "    display(viewer_html(' '.join(['<iframe srcdoc=\"{srcdoc}\"',\n",
        "                            'width=\"{width}px\" height=\"{height}px\"',\n",
        "                            'style=\"border:none;\"></iframe>']).format(\n",
        "                            srcdoc=curr_dm_html, height=256, width=256)))"
      ],
      "metadata": {
        "id": "LSF82eZ6FFO5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Options\n",
        "opt.inference.no_surface_renderings = True # When true, only RGB videos will be created\n",
        "opt.inference.azim_video = True # When true, the camera trajectory will travel along the azimuth direction. Otherwise, the camera will travel along an ellipsoid trajectory.\n",
        "\n",
        "opt.inference.results_dst_dir = os.path.join(os.path.split(opt.inference.results_dst_dir)[0], 'videos')\n",
        "os.makedirs(opt.inference.results_dst_dir, exist_ok=True)\n",
        "render_video(opt.inference, g_ema, surface_g_ema, device, mean_latent, surface_mean_latent)"
      ],
      "metadata": {
        "id": "vjuPNOFJFGAC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "python3 -m https.server 8000"
      ],
      "metadata": {
        "id": "hRCZU3_nFLUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change ffhq1024x1024 to afhq512x512 if you are working on the AFHQ model\n",
        "%%html\n",
        "<div>\n",
        "  <video width=256 controls><source src=\"https://localhost:8000/evaluations/fashion/final_model/videos/sample_video_0_azim.mp4\" type=\"video/mp4\"></video>\n",
        "  <video width=256 controls><source src=\"https://localhost:8000/evaluations/fashion/final_model/videos/sample_video_1_azim.mp4\" type=\"video/mp4\"></video>\n",
        "  <video width=256 controls><source src=\"https://localhost:8000/evaluations/fashion/final_model/videos/sample_video_2_azim.mp4\" type=\"video/mp4\"></video>\n",
        "  <video width=256 controls><source src=\"https://localhost:8000/evaluations/fashion/final_model/videos/sample_video_3_azim.mp4\" type=\"video/mp4\"></video>\n",
        "</div>"
      ],
      "metadata": {
        "id": "_vcdK8QOFOY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_shapes_and_images.py --expname fashion --size 512 --identities 3"
      ],
      "metadata": {
        "id": "LnrPw5K7S0xm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}